{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e10236",
   "metadata": {},
   "source": [
    "\n",
    "# 1) YÃ¼zlerin BlurlanmasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a599ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# === KlasÃ¶rler ===\n",
    "SNAPSHOTS_ROOT = Path(\"snapshots\")  # camera_XXX/YYYY-MM-DD/HH/*.jpg\n",
    "\n",
    "# === S3 Object Storage AyarlarÄ± ===\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3>=1.34.0\", \"-q\"])\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "\n",
    "# S3 AyarlarÄ±\n",
    "S3_ENDPOINT_URL = os.getenv(\"S3_ENDPOINT_URL\", \"https://161cohesity.carrefoursa.com:3000\")\n",
    "S3_ACCESS_KEY_ID = os.getenv(\"S3_ACCESS_KEY_ID\", \"sWxdTl3ERx7myBE1qpW06_haVvuhATcdsmBbqaWkXYU\")\n",
    "S3_SECRET_ACCESS_KEY = os.getenv(\"S3_SECRET_ACCESS_KEY\", \"Ti9Fonk3wYyG5PMx5LaGUmlcVyCuqsE5BLVV5vv8PU0\")\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\", \"Grocery\")\n",
    "\n",
    "_s3_client = None\n",
    "\n",
    "def _ensure_s3_client():\n",
    "    \"\"\"S3 client'Ä± baÅŸlat\"\"\"\n",
    "    global _s3_client\n",
    "    if not S3_ACCESS_KEY_ID or not S3_SECRET_ACCESS_KEY:\n",
    "        return None\n",
    "    if _s3_client is None:\n",
    "        _s3_client = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=S3_ENDPOINT_URL,\n",
    "            aws_access_key_id=S3_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=S3_SECRET_ACCESS_KEY,\n",
    "            verify=False  # Self-signed certificate iÃ§in\n",
    "        )\n",
    "    return _s3_client\n",
    "\n",
    "def _upload_file(local_path: Path, s3_key: str, content_type: str = \"image/jpeg\"):\n",
    "    \"\"\"DosyayÄ± S3'e yÃ¼kle (varsa Ã¼zerine yazar).\"\"\"\n",
    "    s3 = _ensure_s3_client()\n",
    "    if not s3:\n",
    "        return None\n",
    "    try:\n",
    "        with open(local_path, \"rb\") as f:\n",
    "            s3.upload_fileobj(\n",
    "                f,\n",
    "                S3_BUCKET_NAME,\n",
    "                s3_key,\n",
    "                ExtraArgs={'ContentType': content_type}\n",
    "            )\n",
    "        return s3_key\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 upload hatasÄ± ({s3_key}): {e}\")\n",
    "        return None\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "\n",
    "def _to_snapshot_blob_path(local_path: Path) -> str:\n",
    "    \"\"\"SNAPSHOTS_ROOT'e gÃ¶re baÄŸÄ±l yolu blob yoluna Ã§evirir: snapshots/<...>\"\"\"\n",
    "    rel = local_path.relative_to(SNAPSHOTS_ROOT)\n",
    "    return f\"snapshots/{rel.as_posix()}\"\n",
    "\n",
    "\n",
    "def get_latest_folder(camera_path: Path) -> Path | None:\n",
    "    \"\"\"camera_XXX altÄ±nda en yeni tarih/saat klasÃ¶rÃ¼nÃ¼ bulur\"\"\"\n",
    "    if not camera_path.exists():\n",
    "        return None\n",
    "    dates = sorted([d for d in camera_path.iterdir() if d.is_dir()], key=lambda p: p.name)\n",
    "    if not dates:\n",
    "        return None\n",
    "    last_date = dates[-1]\n",
    "    hours = sorted([h for h in last_date.iterdir() if h.is_dir()], key=lambda p: int(p.name))\n",
    "    return hours[-1] if hours else None\n",
    "\n",
    "\n",
    "def blur_faces(img_path: Path):\n",
    "    \"\"\"YÃ¼zleri bulanÄ±klaÅŸtÄ±r, yerinde kaydet ve S3 Object Storage'a aynÄ± yapÄ±yla yÃ¼kle.\"\"\"\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        print(f\"âš ï¸  GÃ¶rsel okunamadÄ±: {img_path}\")\n",
    "        return None\n",
    "\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    boxes, _ = mtcnn.detect(rgb)\n",
    "\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "            face = img[y1:y2, x1:x2]\n",
    "            if face.size == 0:\n",
    "                continue\n",
    "            blurred = cv2.GaussianBlur(face, (51, 51), 30)\n",
    "            img[y1:y2, x1:x2] = blurred\n",
    "\n",
    "    cv2.imwrite(str(img_path), img)\n",
    "\n",
    "    # S3'e yÃ¼kle\n",
    "    s3_key = _to_snapshot_blob_path(img_path)\n",
    "    _upload_file(img_path, s3_key, content_type=\"image/jpeg\")\n",
    "    print(f\"âœ”ï¸  Blur+Upload: {img_path.name} -> {s3_key}\")\n",
    "    return s3_key\n",
    "\n",
    "\n",
    "def process_all_cameras_snapshots():\n",
    "    \"\"\"TÃ¼m kameralar iÃ§in son saat klasÃ¶rÃ¼ndeki gÃ¶rÃ¼ntÃ¼leri blur'la ve S3'e yÃ¼kle.\"\"\"\n",
    "    if not SNAPSHOTS_ROOT.exists():\n",
    "        print(f\"[!] Snapshot kÃ¶kÃ¼ bulunamadÄ±: {SNAPSHOTS_ROOT}\")\n",
    "        return\n",
    "    cameras = [d for d in SNAPSHOTS_ROOT.iterdir() if d.is_dir() and d.name.startswith(\"camera_\")]\n",
    "    for cam in cameras:\n",
    "        latest_hour = get_latest_folder(cam)\n",
    "        if not latest_hour:\n",
    "            print(f\"âŒ Son klasÃ¶r bulunamadÄ±: {cam.name}\")\n",
    "            continue\n",
    "        print(f\"\\nğŸ“· Kamera: {cam.name} | ğŸ“ {latest_hour.relative_to(SNAPSHOTS_ROOT)}\")\n",
    "        for img_file in sorted(latest_hour.glob(\"*.jpg\")):\n",
    "            try:\n",
    "                blur_faces(img_file)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Hata ({img_file.name}): {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_cameras_snapshots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1031f1",
   "metadata": {},
   "source": [
    "\n",
    "# 2) Yolo Modeli ile Meyve Sebzelerin CroplanmasÄ± ve LLM ile RaporlanmasÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b83ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2 - S3 Object Storage Storage Entegrasyonlu Versiyon\n",
    "# Bu dosyayÄ± notebook'un Cell 2'sine kopyalayÄ±n\n",
    "# ============================================================================\n",
    "\n",
    "import os, re, csv, json, base64, mimetypes, tempfile, shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "# ------- ENV -------\n",
    "def _ensure(pkg, pipname=None):\n",
    "    try: return __import__(pkg)\n",
    "    except Exception:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pipname or pkg, \"-q\"])\n",
    "        return __import__(pkg)\n",
    "\n",
    "dotenv = _ensure(\"dotenv\", \"python-dotenv\")\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "dotenv_path = find_dotenv(usecwd=True)\n",
    "if dotenv_path:\n",
    "    load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "# === S3 Object Storage AyarlarÄ± ===\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3>=1.34.0\", \"-q\"])\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "\n",
    "# S3 AyarlarÄ±\n",
    "S3_ENDPOINT_URL = os.getenv(\"S3_ENDPOINT_URL\", \"https://161cohesity.carrefoursa.com:3000\")\n",
    "S3_ACCESS_KEY_ID = os.getenv(\"S3_ACCESS_KEY_ID\", \"sWxdTl3ERx7myBE1qpW06_haVvuhATcdsmBbqaWkXYU\")\n",
    "S3_SECRET_ACCESS_KEY = os.getenv(\"S3_SECRET_ACCESS_KEY\", \"Ti9Fonk3wYyG5PMx5LaGUmlcVyCuqsE5BLVV5vv8PU0\")\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\", \"Grocery\")\n",
    "\n",
    "_s3_client_cell3 = None\n",
    "\n",
    "def _ensure_s3_client_cell3():\n",
    "    \"\"\"S3 client'Ä± baÅŸlat\"\"\"\n",
    "    global _s3_client_cell3\n",
    "    if not S3_ACCESS_KEY_ID or not S3_SECRET_ACCESS_KEY:\n",
    "        print(\"[!] S3_ACCESS_KEY_ID veya S3_SECRET_ACCESS_KEY tanÄ±mlÄ± deÄŸil!\")\n",
    "        return None\n",
    "    if _s3_client_cell3 is None:\n",
    "        _s3_client_cell3 = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=S3_ENDPOINT_URL,\n",
    "            aws_access_key_id=S3_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=S3_SECRET_ACCESS_KEY,\n",
    "            verify=False  # Self-signed certificate iÃ§in\n",
    "        )\n",
    "    return _s3_client_cell3\n",
    "\n",
    "def download_blob_to_path(s3_key: str, local_path: Path) -> bool:\n",
    "    \"\"\"S3'ten dosyayÄ± indir ve lokal path'e kaydet\"\"\"\n",
    "    s3 = _ensure_s3_client_cell3()\n",
    "    if not s3:\n",
    "        return False\n",
    "    try:\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3.download_file(S3_BUCKET_NAME, s3_key, str(local_path))\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 indirme hatasÄ± ({s3_key}): {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_file_to_blob(local_path: Path, s3_key: str, content_type: str = \"image/jpeg\") -> Optional[str]:\n",
    "    \"\"\"DosyayÄ± S3'e yÃ¼kle\"\"\"\n",
    "    s3 = _ensure_s3_client_cell3()\n",
    "    if not s3:\n",
    "        return None\n",
    "    try:\n",
    "        s3.upload_file(\n",
    "            str(local_path),\n",
    "            S3_BUCKET_NAME,\n",
    "            s3_key,\n",
    "            ExtraArgs={'ContentType': content_type}\n",
    "        )\n",
    "        return s3_key\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 upload hatasÄ± ({local_path.name}): {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_bytes_to_blob(data: bytes, s3_key: str, content_type: str = \"application/octet-stream\") -> Optional[str]:\n",
    "    \"\"\"Bytes verisini S3'e yÃ¼kle\"\"\"\n",
    "    s3 = _ensure_s3_client_cell3()\n",
    "    if not s3:\n",
    "        return None\n",
    "    try:\n",
    "        from io import BytesIO\n",
    "        s3.upload_fileobj(\n",
    "            BytesIO(data),\n",
    "            S3_BUCKET_NAME,\n",
    "            s3_key,\n",
    "            ExtraArgs={'ContentType': content_type}\n",
    "        )\n",
    "        return s3_key\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 upload hatasÄ± (bytes): {e}\")\n",
    "        return None\n",
    "\n",
    "def list_blobs_in_path(prefix: str) -> List[str]:\n",
    "    \"\"\"S3'te belirli bir prefix altÄ±ndaki tÃ¼m object'leri listele\"\"\"\n",
    "    s3 = _ensure_s3_client_cell3()\n",
    "    if not s3:\n",
    "        return []\n",
    "    try:\n",
    "        blobs = []\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=prefix)\n",
    "        for page in pages:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    blobs.append(obj['Key'])\n",
    "        return sorted(blobs)\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 listeleme hatasÄ± ({prefix}): {e}\")\n",
    "        return []\n",
    "\n",
    "def get_blob_url(s3_key: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    S3 Object Storage URL'i oluÅŸturur.\n",
    "    Format: https://161cohesity.carrefoursa.com:3000/Grocery/<s3_key>\n",
    "    \"\"\"\n",
    "    if not s3_key:\n",
    "        return None\n",
    "    try:\n",
    "        # S3 endpoint URL'ini kullanarak public URL oluÅŸtur\n",
    "        if S3_ENDPOINT_URL.endswith('/'):\n",
    "            return f\"{S3_ENDPOINT_URL}{S3_BUCKET_NAME}/{s3_key}\"\n",
    "        else:\n",
    "            return f\"{S3_ENDPOINT_URL}/{S3_BUCKET_NAME}/{s3_key}\"\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  S3 URL oluÅŸturma hatasÄ± ({s3_key}): {e}\")\n",
    "        return None\n",
    "\n",
    "def get_snapshot_blob_path_from_local(local_snapshot_path: Path, camera_id: str, date_name: str, hour_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Lokal snapshot path'inden S3 key'ini oluÅŸturur.\n",
    "    Format: snapshots/camera_XXX/YYYY-MM-DD/HH/filename.jpg\n",
    "    \"\"\"\n",
    "    filename = local_snapshot_path.name\n",
    "    return f\"snapshots/{camera_id}/{date_name}/{hour_name}/{filename}\"\n",
    "\n",
    "# --- Ayarlar ---\n",
    "CAMERAS_YAML   = Path(\"multi_camera_system/cameras.yaml\")  # Kamera konfigÃ¼rasyonlarÄ± ve maÄŸaza isimleri\n",
    "MODEL_PATH     = Path(\"best.pt\")  # YOLOv12 model dosyasÄ±\n",
    "TEMP_DIR       = Path(tempfile.mkdtemp(prefix=\"crops_\"))  # GeÃ§ici iÅŸleme klasÃ¶rÃ¼\n",
    "print(f\"[i] GeÃ§ici iÅŸleme klasÃ¶rÃ¼: {TEMP_DIR}\")\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "GRID_COLS  = 5\n",
    "GRID_ROWS  = 5\n",
    "TILE_SIZE  = (256, 256)\n",
    "CAPTION_H  = 28\n",
    "PADDING    = 6\n",
    "FONT_SIZE  = 16\n",
    "FONT_PATH  = os.getenv(\"COLLAGE_FONT\", \"\")\n",
    "\n",
    "MIN_CONF_ROTTEN = float(os.getenv(\"MIN_CONF_ROTTEN\", \"0.85\"))\n",
    "AZURE_ENDPOINT  = (os.getenv(\"AZURE_OPENAI_ENDPOINT\") or \"\").strip()\n",
    "AZURE_API_KEY   = (os.getenv(\"AZURE_OPENAI_API_KEY\") or \"\").strip()\n",
    "DEPLOYMENT      = (os.getenv(\"AZURE_OPENAI_DEPLOYMENT\") or \"gpt-4.1\").strip()\n",
    "\n",
    "# ------- 3rd party -------\n",
    "PIL = _ensure(\"PIL\", \"pillow\")\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "yaml_mod = _ensure(\"yaml\", \"pyyaml\")\n",
    "import yaml\n",
    "\n",
    "def _ensure_openai():\n",
    "    try:\n",
    "        from openai import AzureOpenAI  # type: ignore\n",
    "        return AzureOpenAI\n",
    "    except Exception:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai>=1.30.0\", \"-q\"])\n",
    "        from openai import AzureOpenAI  # type: ignore\n",
    "        return AzureOpenAI\n",
    "AzureOpenAI = _ensure_openai()\n",
    "\n",
    "# ------- Veri tipleri -------\n",
    "@dataclass\n",
    "class Rec:\n",
    "    path: Path\n",
    "    id: str\n",
    "    urun: str\n",
    "    tarih: str\n",
    "    saat: str\n",
    "    magaza: str\n",
    "    snapshot_blob_url: Optional[str] = None  # Kaynak snapshot'Ä±n S3 URL'i\n",
    "\n",
    "# ------- YardÄ±mcÄ±lar -------\n",
    "def load_camera_config(camera_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"cameras.yaml'dan belirli bir kameranÄ±n bilgilerini Ã§ek\"\"\"\n",
    "    if not CAMERAS_YAML.exists(): \n",
    "        return {\"store_name\": \"Bilinmeyen MaÄŸaza\"}\n",
    "    with open(CAMERAS_YAML, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f) or {}\n",
    "    cameras = data.get(\"cameras\", {})\n",
    "    camera = cameras.get(camera_id, {})\n",
    "    return camera\n",
    "\n",
    "def get_store_name(camera_id: str) -> str:\n",
    "    \"\"\"Kamera ID'sine gÃ¶re maÄŸaza ismini al\"\"\"\n",
    "    config = load_camera_config(camera_id)\n",
    "    return config.get(\"store_name\", camera_id or \"Bilinmeyen MaÄŸaza\")\n",
    "\n",
    "DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "HOUR_RE = re.compile(r\"^\\d{1,2}$\")\n",
    "\n",
    "def find_latest_date_hour_for_camera_from_s3(camera_id: str) -> Optional[Tuple[str, str, List[str]]]:\n",
    "    \"\"\"\n",
    "    S3 Object Storage'dan en son tarih/saat klasÃ¶rÃ¼nÃ¼ bulur.\n",
    "    Returns: (date_name, hour_name, s3_keys_list) veya None\n",
    "    S3 key formatÄ±: snapshots/camera_XXX/YYYY-MM-DD/HH/*.jpg\n",
    "    \"\"\"\n",
    "    prefix = f\"snapshots/{camera_id}/\"\n",
    "    all_blobs = list_blobs_in_path(prefix)\n",
    "    \n",
    "    if not all_blobs:\n",
    "        return None\n",
    "    \n",
    "    # Tarih ve saat klasÃ¶rlerini bul\n",
    "    dates = set()\n",
    "    hour_blobs = {}  # (date, hour) -> [blob_paths]\n",
    "    \n",
    "    for blob_path in all_blobs:\n",
    "        # snapshots/camera_001/2025-01-27/17/image.jpg formatÄ±ndan parse et\n",
    "        parts = blob_path.replace(prefix, \"\").split(\"/\")\n",
    "        if len(parts) >= 3 and DATE_RE.match(parts[0]) and HOUR_RE.match(parts[1]):\n",
    "            date_name = parts[0]\n",
    "            hour_name = parts[1]\n",
    "            dates.add(date_name)\n",
    "            key = (date_name, hour_name)\n",
    "            if key not in hour_blobs:\n",
    "                hour_blobs[key] = []\n",
    "            hour_blobs[key].append(blob_path)\n",
    "    \n",
    "    if not dates:\n",
    "        return None\n",
    "    \n",
    "    # En son tarihi bul\n",
    "    sorted_dates = sorted(dates)\n",
    "    last_date = sorted_dates[-1]\n",
    "    \n",
    "    # En son saati bul\n",
    "    last_hour_blobs = {k: v for k, v in hour_blobs.items() if k[0] == last_date}\n",
    "    if not last_hour_blobs:\n",
    "        return None\n",
    "    \n",
    "    # Saat numaralarÄ±na gÃ¶re sÄ±rala\n",
    "    def get_hour_num(key):\n",
    "        try:\n",
    "            return int(key[1])\n",
    "        except:\n",
    "            return -1\n",
    "    \n",
    "    sorted_hours = sorted(last_hour_blobs.keys(), key=get_hour_num)\n",
    "    last_hour_key = sorted_hours[-1]\n",
    "    \n",
    "    return last_hour_key[0], last_hour_key[1], hour_blobs[last_hour_key]\n",
    "\n",
    "def find_all_cameras_from_s3() -> List[str]:\n",
    "    \"\"\"S3 Object Storage'dan tÃ¼m camera_XXX klasÃ¶rlerini bulur\"\"\"\n",
    "    prefix = \"snapshots/camera_\"\n",
    "    all_blobs = list_blobs_in_path(prefix)\n",
    "    \n",
    "    cameras = set()\n",
    "    for blob_path in all_blobs:\n",
    "        # snapshots/camera_001/... formatÄ±ndan camera_001 Ã§Ä±kar\n",
    "        parts = blob_path.split(\"/\")\n",
    "        if len(parts) >= 2 and parts[1].startswith(\"camera_\"):\n",
    "            cameras.add(parts[1])\n",
    "    \n",
    "    return sorted(list(cameras))\n",
    "\n",
    "def _ensure_yolo():\n",
    "    \"\"\"YOLOv12 iÃ§in ultralytics kÃ¼tÃ¼phanesini yÃ¼kle\"\"\"\n",
    "    try:\n",
    "        from ultralytics import YOLO\n",
    "        return YOLO\n",
    "    except Exception:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics>=8.0.0\", \"-q\"])\n",
    "        from ultralytics import YOLO\n",
    "        return YOLO\n",
    "\n",
    "def run_detection_and_crop(snapshot_paths: List[Path], output_dir: Path, \n",
    "                           model_path: Path = MODEL_PATH,\n",
    "                           upload_to_s3: bool = True) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    YOLOv12 ile detection yapÄ±p crop'larÄ± kaydeder ve S3'e yÃ¼kler.\n",
    "    Returns: Her crop iÃ§in bilgiler (crop_path, class_name, confidence, coordinates, ...)\n",
    "    \"\"\"\n",
    "    YOLO = _ensure_yolo()\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model dosyasÄ± bulunamadÄ±: {model_path}\")\n",
    "    \n",
    "    print(f\"[i] Model yÃ¼kleniyor: {model_path}\")\n",
    "    model = YOLO(str(model_path))\n",
    "    \n",
    "    all_crops = []\n",
    "    for snapshot_path in snapshot_paths:\n",
    "        print(f\"[â†’] Detection: {snapshot_path.name}\")\n",
    "        try:\n",
    "            results = model.predict(str(snapshot_path), verbose=False)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Detection hatasÄ± ({snapshot_path.name}): {e}\")\n",
    "            continue\n",
    "            \n",
    "        if not results or len(results) == 0: continue\n",
    "        result = results[0]\n",
    "        if result.boxes is None or len(result.boxes) == 0: continue\n",
    "        \n",
    "        # Her tespit iÃ§in crop\n",
    "        for idx, box in enumerate(result.boxes):\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = float(box.conf[0].cpu().numpy())\n",
    "            cls_id = int(box.cls[0].cpu().numpy())\n",
    "            class_name = result.names[cls_id] if cls_id < len(result.names) else f\"class_{cls_id}\"\n",
    "            \n",
    "            class_dir = output_dir / class_name\n",
    "            class_dir.mkdir(parents=True, exist_ok=True)\n",
    "            crop_filename = f\"{snapshot_path.stem}_{idx:03d}_{conf:.2f}.jpg\"\n",
    "            crop_path = class_dir / crop_filename\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(snapshot_path)\n",
    "                # Bounding box doÄŸrulama\n",
    "                img_width, img_height = img.size\n",
    "                x1 = max(0, int(x1))\n",
    "                y1 = max(0, int(y1))\n",
    "                x2 = min(img_width, int(x2))\n",
    "                y2 = min(img_height, int(y2))\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    print(f\"âš ï¸  GeÃ§ersiz bounding box: ({x1},{y1},{x2},{y2})\")\n",
    "                    continue\n",
    "                    \n",
    "                crop_img = img.crop((x1, y1, x2, y2))\n",
    "                crop_img.save(crop_path, quality=95)\n",
    "                \n",
    "                # S3'e yÃ¼kle\n",
    "                if upload_to_s3:\n",
    "                    # S3 key: crops/camera_XXX/YYYY-MM-DD/HH/class_name/crop_filename.jpg\n",
    "                    # output_dir = TEMP_DIR / \"crops\" / cam_id / date_name / hour_name\n",
    "                    # TEMP_DIR kÄ±smÄ±nÄ± Ã§Ä±kar, sadece crops/... kÄ±smÄ±nÄ± al\n",
    "                    temp_parts = list(crop_path.parts)\n",
    "                    crops_idx = next(i for i, p in enumerate(temp_parts) if p == \"crops\")\n",
    "                    s3_key = \"/\".join(temp_parts[crops_idx:])\n",
    "                    upload_file_to_blob(crop_path, s3_key, content_type=\"image/jpeg\")\n",
    "                \n",
    "                # .txt dosyasÄ± da kaydet (opsiyonel, S3'e yÃ¼kleme)\n",
    "                txt_path = crop_path.with_suffix(\".txt\")\n",
    "                with open(txt_path, \"w\") as f:\n",
    "                    f.write(f\"{cls_id} {conf:.6f} {x1} {y1} {x2} {y2}\\n\")\n",
    "                \n",
    "                all_crops.append({\n",
    "                    \"crop_path\": crop_path,\n",
    "                    \"snapshot_path\": snapshot_path,\n",
    "                    \"class_name\": class_name,\n",
    "                    \"class_id\": cls_id,\n",
    "                    \"confidence\": conf,\n",
    "                    \"bbox\": [float(x1), float(y1), float(x2), float(y2)]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Crop iÅŸleme hatasÄ± ({snapshot_path.name}, {idx}): {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"[âœ“] Toplam {len(all_crops)} crop oluÅŸturuldu\")\n",
    "    return all_crops\n",
    "\n",
    "def chunked(seq, n: int):\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i+n]\n",
    "\n",
    "# Pillow resampling fallback\n",
    "try:\n",
    "    RESAMPLE = Image.Resampling.LANCZOS\n",
    "except Exception:\n",
    "    RESAMPLE = Image.LANCZOS if hasattr(Image, \"LANCZOS\") else Image.ANTIALIAS\n",
    "\n",
    "def load_font():\n",
    "    try:\n",
    "        if FONT_PATH and Path(FONT_PATH).exists():\n",
    "            return ImageFont.truetype(FONT_PATH, FONT_SIZE)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "def _text_size(draw: ImageDraw.ImageDraw, text: str, font: ImageFont.ImageFont):\n",
    "    if hasattr(draw, \"textbbox\"):\n",
    "        x0,y0,x1,y1 = draw.textbbox((0,0), text, font=font)\n",
    "        return x1-x0, y1-y0\n",
    "    try:\n",
    "        return font.getsize(text)  # type: ignore\n",
    "    except Exception:\n",
    "        return (len(text)*8, FONT_SIZE)\n",
    "\n",
    "def draw_caption_bar(draw, xy, text, font):\n",
    "    x1,y1,x2,y2 = xy\n",
    "    draw.rectangle([x1,y1,x2,y2], fill=(0,0,0))\n",
    "    tw, th = _text_size(draw, text, font)\n",
    "    tx = x1 + (x2-x1-tw)//2\n",
    "    ty = y1 + (y2-y1-th)//2\n",
    "    draw.text((tx,ty), text, fill=(255,255,255), font=font)\n",
    "\n",
    "def make_collage(batch: List['Rec'], out_dir: Path, index: int, upload_to_s3: bool = True) -> Tuple[Path, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Collage oluÅŸtur ve S3'e yÃ¼kle.\n",
    "    Collage'lar collages klasÃ¶rÃ¼ne kaydedilir (out_dir = crops_dir / \"collages\").\n",
    "    JSON dosyalarÄ± da aynÄ± klasÃ¶re kaydedilir.\n",
    "    Returns: (local_path, s3_blob_path)\n",
    "    \"\"\"\n",
    "    W = GRID_COLS * (TILE_SIZE[0] + 2*PADDING)\n",
    "    H = GRID_ROWS * (TILE_SIZE[1] + CAPTION_H + 2*PADDING)\n",
    "    canvas = Image.new(\"RGB\", (W,H), (30,30,30))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    font = load_font()\n",
    "\n",
    "    for i, rec in enumerate(batch):\n",
    "        r = i // GRID_COLS\n",
    "        c = i % GRID_COLS\n",
    "        x0 = c * (TILE_SIZE[0] + 2*PADDING) + PADDING\n",
    "        y0 = r * (TILE_SIZE[1] + CAPTION_H + 2*PADDING) + PADDING\n",
    "        try:\n",
    "            im = Image.open(rec.path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Collage gÃ¶rÃ¼ntÃ¼ yÃ¼klenemedi: {rec.path} - {e}\")\n",
    "            im = Image.new(\"RGB\", TILE_SIZE, (80,80,80))\n",
    "        im = im.copy()\n",
    "        im.thumbnail(TILE_SIZE, RESAMPLE)\n",
    "        tile = Image.new(\"RGB\", TILE_SIZE, (20,20,20))\n",
    "        ox = (TILE_SIZE[0]-im.size[0])//2\n",
    "        oy = (TILE_SIZE[1]-im.size[1])//2\n",
    "        tile.paste(im, (ox,oy))\n",
    "        canvas.paste(tile, (x0,y0))\n",
    "\n",
    "        num = str(i+1)\n",
    "        cap_y1 = y0 + TILE_SIZE[1]\n",
    "        cap_y2 = cap_y1 + CAPTION_H\n",
    "        draw_caption_bar(draw, (x0,cap_y1,x0+TILE_SIZE[0],cap_y2), num, font)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"collage_{index:03d}.jpg\"\n",
    "    canvas.save(out_path, quality=88, optimize=True)\n",
    "    \n",
    "    # S3'e yÃ¼kle (collages klasÃ¶rÃ¼ne)\n",
    "    s3_path = None\n",
    "    if upload_to_s3:\n",
    "        # out_dir = TEMP_DIR / \"crops\" / cam_id / date_name / hour_name / \"collages\"\n",
    "        # S3 key: crops/camera_XXX/YYYY-MM-DD/HH/collages/collage_XXX.jpg\n",
    "        # TEMP_DIR kÄ±smÄ±nÄ± Ã§Ä±kar, sadece crops/... kÄ±smÄ±nÄ± al\n",
    "        temp_parts = list(out_path.parts)\n",
    "        crops_idx = next(i for i, p in enumerate(temp_parts) if p == \"crops\")\n",
    "        s3_key = \"/\".join(temp_parts[crops_idx:])\n",
    "        s3_path = upload_file_to_blob(out_path, s3_key, content_type=\"image/jpeg\")\n",
    "    \n",
    "    return out_path, s3_path\n",
    "\n",
    "def guess_mime(path: Path) -> str:\n",
    "    mt, _ = mimetypes.guess_type(str(path))\n",
    "    return mt or \"image/jpeg\"\n",
    "\n",
    "def b64_image(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def prompt_rotten_only(batch_size: int, min_conf: float, num_to_class: Dict[int, str]) -> str:\n",
    "    mapping = \", \".join([f\"{i}={cls}\" for i, cls in num_to_class.items()])\n",
    "    return (\n",
    "        f\"Bu gÃ¶rsel {batch_size} kutudan oluÅŸan tek bir kolajdÄ±r. \"\n",
    "        \"Her kutuda 1'den baÅŸlayarak artan sÄ±ra numarasÄ± vardÄ±r. \"\n",
    "        f\"Numaraâ†’ÃœrÃ¼n eÅŸleÅŸmesi: {mapping}.\\n\\n\"\n",
    "        \"YalnÄ±zca ilgili numarada belirtilen Ã¼rÃ¼n tÃ¼rÃ¼yle aÃ§Ä±kÃ§a eÅŸleÅŸen kareleri deÄŸerlendir. \"\n",
    "        \"ÃœrÃ¼n tÃ¼rÃ¼ belirgin ÅŸekilde farklÄ±ysa veya meyve/sebze dÄ±ÅŸÄ± bir nesne baskÄ±nsa, o kareyi deÄŸerlendirme kapsamÄ± dÄ±ÅŸÄ±nda bÄ±rak. \"\n",
    "        \"Ä°nsan, yÃ¼z, vÃ¼cut parÃ§asÄ±, metin, silah, yaralanma vb. iÃ§erik sezersen hiÃ§bir deÄŸerlendirme yapma.\\n\\n\"\n",
    "        \"DeÄŸerlendirme: Belirgin Ã§Ã¼rÃ¼kleri iÅŸaretle; kÃ¼Ã§Ã¼k lekeler/olgunlaÅŸma taze kabul. \"\n",
    "        f\"Sadece gÃ¼venin {min_conf:.2f} ve Ã¼zerindeyse Ã§Ã¼rÃ¼k olarak bildir. \"\n",
    "        \"Ã‡Ä±ktÄ± formatÄ± tek bir JSON nesnesi olmalÄ± ve ÅŸu yapÄ±yÄ± iÃ§ermelidir:\\n\"\n",
    "        \"{\\\"rotten\\\": [{\\\"id\\\": <1..N>, \\\"guven\\\": 0..1}]}\\n\"\n",
    "        \"Listede olmayan kareler taze kabul edilir.\"\n",
    "    )\n",
    "\n",
    "def _ensure_openai_client():\n",
    "    return AzureOpenAI(api_key=AZURE_API_KEY, api_version=\"2024-06-01\", azure_endpoint=AZURE_ENDPOINT)\n",
    "\n",
    "SAFE_SYSTEM_MESSAGE = (\n",
    "    \"You are an assistant that evaluates produce freshness in retail photos. \"\n",
    "    \"Analyze only non-sensitive, non-human content (fruits/vegetables in store crates). \"\n",
    "    \"Do not discuss or infer anything about people, faces, body parts, weapons, injuries, or text content. \"\n",
    "    \"If such content appears, return a neutral JSON with no items marked rotten.\"\n",
    ")\n",
    "\n",
    "def azure_classify_collage(client, deployment: str, collage_path: Path, batch_size: int,\n",
    "                           num_to_class: Dict[int, str], min_conf: float, \n",
    "                           batch_records: List = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Azure OpenAI ile collage analizi.\n",
    "    Not: Content-filter/LLM hatasÄ±nda tek tek gÃ¶rsel test/ayÄ±klama YOK, kolaj atlanÄ±r.\n",
    "    \"\"\"\n",
    "    b64 = b64_image(collage_path)\n",
    "    user_text = prompt_rotten_only(batch_size, min_conf, num_to_class)\n",
    "    base_messages = [\n",
    "        {\"role\": \"system\", \"content\": SAFE_SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_text},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{guess_mime(collage_path)};base64,{b64}\"}}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    def _call(messages):\n",
    "        return client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=1000,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "\n",
    "    # 1. deneme\n",
    "    try:\n",
    "        resp = _call(base_messages)\n",
    "    except Exception as e1:\n",
    "        # 2. deneme\n",
    "        try:\n",
    "            safer = [\n",
    "                {\"role\": \"system\", \"content\": SAFE_SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_text},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{guess_mime(collage_path)};base64,{b64}\"}}\n",
    "                ]}\n",
    "            ]\n",
    "            resp = _call(safer)\n",
    "        except Exception as e2:\n",
    "            error_log = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error_primary\": str(e1),\n",
    "                \"error_secondary\": str(e2),\n",
    "                \"collage_path\": str(collage_path),\n",
    "                \"batch_size\": batch_size\n",
    "            }\n",
    "            err_file = collage_path.with_suffix(\".llm_error.json\")\n",
    "            with open(err_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(error_log, f, ensure_ascii=False, indent=2)\n",
    "            return {\"rotten\": [], \"error\": \"llm_error\", \"skipped\": True}\n",
    "\n",
    "    # JSON parse\n",
    "    try:\n",
    "        j = json.loads(resp.choices[0].message.content)\n",
    "        return j if isinstance(j, dict) and \"rotten\" in j else {\"rotten\": []}\n",
    "    except Exception:\n",
    "        return {\"rotten\": []}\n",
    "\n",
    "def gather_crops_for_hour(crops_dir: Path, magaza: str, date_name: str, hour_name: str, \n",
    "                          crop_data: List[Dict[str, Any]], camera_id: str) -> List[Rec]:\n",
    "    \"\"\"\n",
    "    Crop klasÃ¶rÃ¼ndeki tÃ¼m jpg dosyalarÄ±nÄ± Rec objesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.\n",
    "    Format: crops/<kamera>/<tarih>/<saat>/<class_name>/*.jpg\n",
    "    \n",
    "    Args:\n",
    "        crop_data: run_detection_and_crop'dan dÃ¶nen crop bilgileri (snapshot_path iÃ§erir)\n",
    "        camera_id: Kamera ID'si (snapshot blob path iÃ§in)\n",
    "    \"\"\"\n",
    "    exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n",
    "    recs: List[Rec] = []\n",
    "    excluded_dirs = {\"collages\", \".git\", \"__pycache__\"}  # collages klasÃ¶rÃ¼nÃ¼ atla\n",
    "    \n",
    "    # Crop path -> snapshot_path mapping oluÅŸtur\n",
    "    crop_to_snapshot = {}\n",
    "    for crop_info in crop_data:\n",
    "        crop_path = crop_info.get(\"crop_path\")\n",
    "        snapshot_path = crop_info.get(\"snapshot_path\")\n",
    "        if crop_path and snapshot_path:\n",
    "            crop_to_snapshot[crop_path] = snapshot_path\n",
    "    \n",
    "    for class_dir in sorted([d for d in crops_dir.iterdir() \n",
    "                             if d.is_dir() and d.name not in excluded_dirs]):\n",
    "        urun = class_dir.name\n",
    "        for p in sorted(class_dir.glob(\"*\")):\n",
    "            if p.suffix.lower() in exts:\n",
    "                # Bu crop'un kaynak snapshot'Ä±nÄ± bul\n",
    "                snapshot_path = crop_to_snapshot.get(p)\n",
    "                snapshot_blob_url = None\n",
    "                \n",
    "                if snapshot_path:\n",
    "                    # Snapshot'Ä±n S3 key'ini oluÅŸtur\n",
    "                    snapshot_s3_key = get_snapshot_blob_path_from_local(\n",
    "                        snapshot_path, camera_id, date_name, hour_name\n",
    "                    )\n",
    "                    # S3 URL'ini al\n",
    "                    snapshot_blob_url = get_blob_url(snapshot_s3_key)\n",
    "                \n",
    "                recs.append(Rec(\n",
    "                    path=p, \n",
    "                    id=p.stem, \n",
    "                    urun=urun, \n",
    "                    tarih=date_name,\n",
    "                    saat=hour_name.zfill(2) + \":00:00\", \n",
    "                    magaza=magaza,\n",
    "                    snapshot_blob_url=snapshot_blob_url\n",
    "                ))\n",
    "    return recs\n",
    "\n",
    "def write_csv(path: Path, rows: List[Dict]):\n",
    "    if not rows: return\n",
    "    fieldnames = set()\n",
    "    for r in rows: fieldnames.update(r.keys())\n",
    "    ordered = [\"id\",\"urun\",\"magaza\",\"tarih\",\"saat\",\"durum\",\"guven\",\"dosya\"]\n",
    "    for f in sorted(fieldnames):\n",
    "        if f not in ordered: ordered.append(f)\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=ordered); w.writeheader(); w.writerows(rows)\n",
    "\n",
    "def write_summary(path: Path, rows: List[Dict]):\n",
    "    total = len(rows)\n",
    "    rotten = sum(1 for r in rows if r.get(\"durum\") == \"Ã§Ã¼rÃ¼k\")\n",
    "    fresh  = total - rotten\n",
    "    rate = (rotten / total * 100.0) if total > 0 else 0.0\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Toplam: {total}\\nÃ‡Ã¼rÃ¼k : {rotten}\\nSaÄŸlÄ±klÄ±: {fresh}\\nÃ‡Ã¼rÃ¼k OranÄ±: {rate:.2f}%\\n\")\n",
    "\n",
    "def main(camera_id: str = None):\n",
    "    \"\"\"\n",
    "    Ana iÅŸlem: S3'ten snapshot'larÄ± indirir, iÅŸler, \n",
    "    crop'lar, LLM analizi yapar ve tÃ¼m Ã§Ä±ktÄ±larÄ± S3'e yÃ¼kler.\n",
    "    \"\"\"\n",
    "    if not AZURE_ENDPOINT or not AZURE_API_KEY:\n",
    "        raise RuntimeError(\"AZURE_OPENAI_ENDPOINT / AZURE_OPENAI_API_KEY eksik.\")\n",
    "    \n",
    "    # Hangi kamera(lar) iÅŸlenecek?\n",
    "    if camera_id:\n",
    "        cameras_to_process = [camera_id]\n",
    "    else:\n",
    "        cameras_to_process = find_all_cameras_from_s3()\n",
    "        if not cameras_to_process:\n",
    "            print(\"[!] S3 Object Storage'da hiÃ§ kamera klasÃ¶rÃ¼ bulunamadÄ±\")\n",
    "            return\n",
    "    \n",
    "    print(f\"[i] Ä°ÅŸlenecek kameralar: {', '.join(cameras_to_process)}\")\n",
    "    \n",
    "    try:\n",
    "        for cam_id in cameras_to_process:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"[â†’] {cam_id} iÅŸleniyor...\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            magaza = get_store_name(cam_id)\n",
    "            print(f\"[i] MaÄŸaza: {magaza}\")\n",
    "            \n",
    "            # S3'ten en son tarih/saat bilgisini al\n",
    "            found = find_latest_date_hour_for_camera_from_s3(cam_id)\n",
    "            if not found:\n",
    "                print(f\"[!] {cam_id} iÃ§in snapshot bulunamadÄ±.\")\n",
    "                continue\n",
    "            \n",
    "            date_name, hour_name, s3_keys = found\n",
    "            print(f\"[i] Son klasÃ¶r: {date_name}/{hour_name}\")\n",
    "            print(f\"[i] Toplam snapshot: {len(s3_keys)}\")\n",
    "            \n",
    "            # GeÃ§ici lokal klasÃ¶r oluÅŸtur\n",
    "            temp_snapshots_dir = TEMP_DIR / cam_id / date_name / hour_name\n",
    "            temp_snapshots_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # S3'ten snapshot'larÄ± indir\n",
    "            print(f\"[â†’] S3'ten snapshot'lar indiriliyor...\")\n",
    "            snapshot_files = []\n",
    "            for s3_key in s3_keys:\n",
    "                if not s3_key.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    continue\n",
    "                filename = s3_key.split('/')[-1]\n",
    "                local_path = temp_snapshots_dir / filename\n",
    "                if download_blob_to_path(s3_key, local_path):\n",
    "                    snapshot_files.append(local_path)\n",
    "            \n",
    "            if not snapshot_files:\n",
    "                print(f\"[!] {cam_id} - Snapshot indirilemedi\")\n",
    "                continue\n",
    "            \n",
    "            snapshot_files.sort()\n",
    "            print(f\"[âœ“] {len(snapshot_files)} snapshot indirildi\")\n",
    "            \n",
    "            # Crop klasÃ¶rÃ¼: TEMP_DIR/crops/camera_XXX/YYYY-MM-DD/HH/\n",
    "            crops_dir = TEMP_DIR / \"crops\" / cam_id / date_name / hour_name\n",
    "            crops_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # YOLOv12 detection ve cropping (S3'e yÃ¼kleme ile)\n",
    "            print(f\"[â†’] YOLOv12 detection baÅŸlÄ±yor...\")\n",
    "            crop_data = run_detection_and_crop(snapshot_files, crops_dir, MODEL_PATH, upload_to_s3=True)\n",
    "            if not crop_data:\n",
    "                print(f\"[!] {cam_id} - HiÃ§ tespit yapÄ±lamadÄ±\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"[i] Toplam crop: {len(crop_data)}\")\n",
    "            \n",
    "            # Rec listesi (crop_data'yÄ± geÃ§erek snapshot bilgisini de al)\n",
    "            recs = gather_crops_for_hour(crops_dir, magaza, date_name, hour_name, crop_data, cam_id)\n",
    "            if not recs:\n",
    "                print(f\"[!] {cam_id} - Crop yok\")\n",
    "                continue\n",
    "            \n",
    "            client = _ensure_openai_client()\n",
    "            # Collage'larÄ± ayrÄ± bir collages klasÃ¶rÃ¼ne kaydet\n",
    "            collages_dir = crops_dir / \"collages\"\n",
    "            collages_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            all_items: List[Dict] = []\n",
    "            batch_idx = 1\n",
    "            \n",
    "            for batch in chunked(recs, BATCH_SIZE):\n",
    "                print(f\"[â†’] Batch {batch_idx} iÅŸleniyor ({len(batch)} crop)...\")\n",
    "                collage_path, collage_s3_path = make_collage(batch, collages_dir, batch_idx, upload_to_s3=True)\n",
    "                num_to_class = {i + 1: rec.urun for i, rec in enumerate(batch)}\n",
    "                \n",
    "                # LLM Ã§aÄŸrÄ±sÄ±\n",
    "                result = azure_classify_collage(\n",
    "                    client, DEPLOYMENT, collage_path, len(batch), \n",
    "                    num_to_class, MIN_CONF_ROTTEN, batch\n",
    "                )\n",
    "                \n",
    "                if result.get(\"skipped\"):\n",
    "                    print(f\"âš ï¸  Batch {batch_idx} atlandÄ± (LLM hatasÄ±).\")\n",
    "                    batch_idx += 1\n",
    "                    continue\n",
    "                \n",
    "                # rotten list â†’ indeks: gÃ¼ven\n",
    "                rotten_pos: Dict[int, float] = {}\n",
    "                for x in result.get(\"rotten\", []) if isinstance(result, dict) else []:\n",
    "                    try:\n",
    "                        idx = int(x.get(\"id\")); conf = float(x.get(\"guven\", 0.0))\n",
    "                        if 1 <= idx <= len(batch) and conf >= MIN_CONF_ROTTEN:\n",
    "                            rotten_pos[idx] = conf\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                \n",
    "                # satÄ±rlar\n",
    "                batch_items: List[Dict] = []\n",
    "                for i, rec in enumerate(batch, start=1):\n",
    "                    # dosya alanÄ±na kaynak snapshot'Ä±n S3 blob URL'ini yaz\n",
    "                    dosya_url = rec.snapshot_blob_url or str(rec.path)  # Fallback olarak lokal path\n",
    "                    \n",
    "                    row = {\n",
    "                        \"id\": rec.id,\n",
    "                        \"urun\": rec.urun,\n",
    "                        \"magaza\": rec.magaza,\n",
    "                        \"tarih\": rec.tarih,\n",
    "                        \"saat\":  rec.saat,\n",
    "                        \"dosya\": dosya_url  # S3 blob URL veya fallback path\n",
    "                    }\n",
    "                    if i in rotten_pos:\n",
    "                        row.update({\"durum\": \"Ã§Ã¼rÃ¼k\", \"guven\": rotten_pos[i]})\n",
    "                    else:\n",
    "                        row.update({\"durum\": \"saÄŸlÄ±klÄ±\"})\n",
    "                    batch_items.append(row)\n",
    "                \n",
    "                # kolaj JSON dÃ¶kÃ¼mÃ¼ (ham LLM + maddeler)\n",
    "                llm_dump = {\n",
    "                    \"collage_path\": str(collage_path),\n",
    "                    \"collage_blob_path\": collage_s3_path,\n",
    "                    \"batch_size\": len(batch),\n",
    "                    \"model_name\": DEPLOYMENT,\n",
    "                    \"prompt_version\": \"rotten_only_v2_safe\",\n",
    "                    \"latency_ms\": None,\n",
    "                    \"num_to_class\": num_to_class,\n",
    "                    \"min_conf_rotten\": MIN_CONF_ROTTEN,\n",
    "                    \"raw_llm\": result,\n",
    "                    \"items\": batch_items\n",
    "                }\n",
    "                out_llm = collage_path.with_suffix(\".llm.json\")\n",
    "                with open(out_llm, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(llm_dump, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                # LLM JSON'u S3'e yÃ¼kle (collage ile aynÄ± dizinde)\n",
    "                temp_parts = list(out_llm.parts)\n",
    "                crops_idx = next(i for i, p in enumerate(temp_parts) if p == \"crops\")\n",
    "                s3_llm_key = \"/\".join(temp_parts[crops_idx:])\n",
    "                upload_file_to_blob(out_llm, s3_llm_key, content_type=\"application/json\")\n",
    "                \n",
    "                all_items.extend(batch_items)\n",
    "                print(f\"[OK] Batch {batch_idx} -> {collage_path.name} (Ã§Ã¼rÃ¼k={len(rotten_pos)}/{len(batch)})\")\n",
    "                batch_idx += 1\n",
    "            \n",
    "            # RaporlarÄ± kaydet ve S3'e yÃ¼kle\n",
    "            out_json = crops_dir / \"report_all.json\"\n",
    "            with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_items, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            out_csv = crops_dir / \"report_all.csv\"\n",
    "            write_csv(out_csv, all_items)\n",
    "            \n",
    "            out_summary = crops_dir / \"summary.txt\"\n",
    "            write_summary(out_summary, all_items)\n",
    "            \n",
    "            # RaporlarÄ± S3'e yÃ¼kle\n",
    "            def get_s3_key(local_path: Path) -> str:\n",
    "                temp_parts = list(local_path.parts)\n",
    "                crops_idx = next(i for i, p in enumerate(temp_parts) if p == \"crops\")\n",
    "                return \"/\".join(temp_parts[crops_idx:])\n",
    "            \n",
    "            s3_json_key = get_s3_key(out_json)\n",
    "            upload_file_to_blob(out_json, s3_json_key, content_type=\"application/json\")\n",
    "            \n",
    "            s3_csv_key = get_s3_key(out_csv)\n",
    "            upload_file_to_blob(out_csv, s3_csv_key, content_type=\"text/csv\")\n",
    "            \n",
    "            s3_summary_key = get_s3_key(out_summary)\n",
    "            upload_file_to_blob(out_summary, s3_summary_key, content_type=\"text/plain\")\n",
    "            \n",
    "            print(f\"[âœ“] {cam_id} tamamlandÄ±\")\n",
    "            print(f\"    - Crop'lar: {len(crop_data)} tespit (S3'e yÃ¼klendi)\")\n",
    "            print(f\"    - Collage'lar: {batch_idx-1} adet (S3'e yÃ¼klendi)\")\n",
    "            print(f\"    - Raporlar: report_all.json, report_all.csv, summary.txt (S3'e yÃ¼klendi)\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"[âœ“] TÃ¼m kameralar iÅŸlendi!\")\n",
    "        print(f\"[i] GeÃ§ici klasÃ¶r korunuyor: {TEMP_DIR}\")\n",
    "        print(f\"[i] Ä°sterseniz manuel olarak silebilirsiniz: shutil.rmtree('{TEMP_DIR}')\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    finally:\n",
    "        # GeÃ§ici snapshot klasÃ¶rÃ¼nÃ¼ temizle (crops kalacak, Ã§Ã¼nkÃ¼ S3'e yÃ¼klendi)\n",
    "        temp_snapshots_path = TEMP_DIR\n",
    "        if temp_snapshots_path.exists():\n",
    "            # Sadece snapshot klasÃ¶rlerini temizle, crops'u bÄ±rak\n",
    "            for cam_dir in temp_snapshots_path.glob(\"camera_*\"):\n",
    "                if cam_dir.is_dir():\n",
    "                    print(f\"[i] GeÃ§ici snapshot klasÃ¶rÃ¼ temizleniyor: {cam_dir}\")\n",
    "                    shutil.rmtree(cam_dir, ignore_errors=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Tek kamera iÅŸle: main(\"camera_001\")\n",
    "    # TÃ¼m kameralarÄ± iÅŸle: main()\n",
    "    main()  # TÃ¼m kameralar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b1e402",
   "metadata": {},
   "source": [
    "## 3) Veriyi DB'ye aktarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PostgreSQL VeritabanÄ±na Veri AktarÄ±mÄ± (Azure Blob Storage)\n",
      "======================================================================\n",
      "[OK] VeritabanÄ± tablolarÄ± hazÄ±r (llm_runs, llm_items)\n",
      "[i] 2 kamera klasÃ¶rÃ¼ bulundu: camera_001, camera_002\n",
      "\n",
      "[i] 2 kamera iÃ§in iÅŸlem yapÄ±lacak\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[â†’] camera_001: 2025-10-30/14\n",
      "======================================================================\n",
      "[i] camera_001: 10 adet .llm.json dosyasÄ± bulundu\n",
      "\n",
      "[âœ“] RUN BAÅARILI - ID: 32\n",
      "  ğŸ“Š Toplam Kolaj     : 10\n",
      "  ğŸ“¦ Toplam ÃœrÃ¼n      : 232\n",
      "  âœ… SaÄŸlÄ±klÄ±         : 216 (93.1%)\n",
      "  âŒ Ã‡Ã¼rÃ¼k            : 16 (6.9%)\n",
      "  â“ Bilinmeyen       : 0\n",
      "  ğŸ’¾ VeritabanÄ± KayÄ±t : 232 item\n",
      "\n",
      "======================================================================\n",
      "[â†’] camera_002: 2025-10-24/17\n",
      "======================================================================\n",
      "[i] camera_002: 36 adet .llm.json dosyasÄ± bulundu\n",
      "\n",
      "[âœ“] RUN BAÅARILI - ID: 33\n",
      "  ğŸ“Š Toplam Kolaj     : 36\n",
      "  ğŸ“¦ Toplam ÃœrÃ¼n      : 899\n",
      "  âœ… SaÄŸlÄ±klÄ±         : 821 (91.3%)\n",
      "  âŒ Ã‡Ã¼rÃ¼k            : 78 (8.7%)\n",
      "  â“ Bilinmeyen       : 0\n",
      "  ğŸ’¾ VeritabanÄ± KayÄ±t : 899 item\n",
      "\n",
      "======================================================================\n",
      "[âœ“] TÃœM KAMERALAR TAMAMLANDI!\n",
      "======================================================================\n",
      "  ğŸ“Š Toplam Run       : 2\n",
      "  ğŸ“¦ Toplam Item      : 1131\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3 (Cell 5) - PostgreSQL VeritabanÄ±na Veri AktarÄ±mÄ± - S3 Object Storage Storage Entegrasyonlu\n",
    "# Bu dosyayÄ± notebook'un Cell 3'sine (DB aktarma hÃ¼cresi) kopyalayÄ±n\n",
    "# ============================================================================\n",
    "\n",
    "import os, json, re, tempfile\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# --------- Lazy deps ----------\n",
    "def _ensure(pkg, pipname=None):\n",
    "    try: return __import__(pkg)\n",
    "    except Exception:\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pipname or pkg, \"-q\"])\n",
    "        return __import__(pkg)\n",
    "\n",
    "dotenv = _ensure(\"dotenv\", \"python-dotenv\")\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "psycopg2 = _ensure(\"psycopg2\", \"psycopg2-binary\")\n",
    "import psycopg2  # type: ignore\n",
    "\n",
    "# === S3 Object Storage AyarlarÄ± ===\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3>=1.34.0\", \"-q\"])\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "\n",
    "# --------- ENV ----------\n",
    "dotenv_path = find_dotenv(usecwd=True)\n",
    "if dotenv_path:\n",
    "    load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "CAMERAS_YAML = Path(\"multi_camera_system/cameras.yaml\")  # Kamera isimleri iÃ§in\n",
    "\n",
    "# PostgreSQL VeritabanÄ± AyarlarÄ±\n",
    "PG_HOST = os.getenv(\"PG_HOST\", \"45.84.18.76\")\n",
    "PG_USER = os.getenv(\"PG_USER\", \"grocerryadmin\")\n",
    "PG_PASSWORD = os.getenv(\"PG_PASSWORD\", \"a08Iyr95vLHTYY\")\n",
    "PG_DATABASE = os.getenv(\"PG_DATABASE\", \"grocerryadmin\")  # VarsayÄ±lan olarak kullanÄ±cÄ± adÄ± ile aynÄ±\n",
    "PG_PORT = os.getenv(\"PG_PORT\", \"5432\")\n",
    "\n",
    "# PG_DSN oluÅŸtur (environment variable'dan veya yukarÄ±daki bilgilerden)\n",
    "PG_DSN = os.getenv(\"PG_DSN\", f\"postgresql://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}\").strip()\n",
    "\n",
    "# S3 AyarlarÄ±\n",
    "S3_ENDPOINT_URL = os.getenv(\"S3_ENDPOINT_URL\", \"https://161cohesity.carrefoursa.com:3000\")\n",
    "S3_ACCESS_KEY_ID = os.getenv(\"S3_ACCESS_KEY_ID\", \"sWxdTl3ERx7myBE1qpW06_haVvuhATcdsmBbqaWkXYU\")\n",
    "S3_SECRET_ACCESS_KEY = os.getenv(\"S3_SECRET_ACCESS_KEY\", \"Ti9Fonk3wYyG5PMx5LaGUmlcVyCuqsE5BLVV5vv8PU0\")\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\", \"Grocery\")\n",
    "\n",
    "_s3_client_db = None\n",
    "\n",
    "def _ensure_s3_client_db():\n",
    "    \"\"\"S3 client'Ä± baÅŸlat\"\"\"\n",
    "    global _s3_client_db\n",
    "    if not S3_ACCESS_KEY_ID or not S3_SECRET_ACCESS_KEY:\n",
    "        print(\"[!] S3_ACCESS_KEY_ID veya S3_SECRET_ACCESS_KEY tanÄ±mlÄ± deÄŸil!\")\n",
    "        return None\n",
    "    if _s3_client_db is None:\n",
    "        _s3_client_db = boto3.client(\n",
    "            's3',\n",
    "            endpoint_url=S3_ENDPOINT_URL,\n",
    "            aws_access_key_id=S3_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=S3_SECRET_ACCESS_KEY,\n",
    "            verify=False  # Self-signed certificate iÃ§in\n",
    "        )\n",
    "    return _s3_client_db\n",
    "\n",
    "def download_blob_to_memory(s3_key: str) -> Optional[bytes]:\n",
    "    \"\"\"S3'ten dosyayÄ± memory'ye indir\"\"\"\n",
    "    s3 = _ensure_s3_client_db()\n",
    "    if not s3:\n",
    "        return None\n",
    "    try:\n",
    "        from io import BytesIO\n",
    "        buffer = BytesIO()\n",
    "        s3.download_fileobj(S3_BUCKET_NAME, s3_key, buffer)\n",
    "        buffer.seek(0)\n",
    "        return buffer.read()\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 indirme hatasÄ± ({s3_key}): {e}\")\n",
    "        return None\n",
    "\n",
    "def list_blobs_in_path_db(prefix: str) -> List[str]:\n",
    "    \"\"\"S3'te belirli bir prefix altÄ±ndaki tÃ¼m object'leri listele\"\"\"\n",
    "    s3 = _ensure_s3_client_db()\n",
    "    if not s3:\n",
    "        return []\n",
    "    try:\n",
    "        blobs = []\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=prefix)\n",
    "        for page in pages:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    blobs.append(obj['Key'])\n",
    "        return sorted(blobs)\n",
    "    except ClientError as e:\n",
    "        print(f\"âš ï¸  S3 listeleme hatasÄ± ({prefix}): {e}\")\n",
    "        return []\n",
    "\n",
    "# PG_DSN kontrolÃ¼ - eÄŸer yoksa uyarÄ± ver ama hatayÄ± hemen atma\n",
    "if not PG_DSN:\n",
    "    print(\"[UYARI] PG_DSN ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil!\")\n",
    "    print(\"Ã–rnek: postgresql://USER:PASS@HOST:5432/DB?sslmode=require\")\n",
    "    print(\"VeritabanÄ± iÅŸlemleri iÃ§in .env dosyasÄ±na PG_DSN ekleyin.\")\n",
    "else:\n",
    "    if \"sslmode=\" not in PG_DSN:\n",
    "        PG_DSN += (\"&\" if \"?\" in PG_DSN else \"?\") + \"sslmode=require\"\n",
    "\n",
    "# --------- DDL ----------\n",
    "DDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS llm_runs (\n",
    "  id              BIGSERIAL PRIMARY KEY,\n",
    "  created_at      TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n",
    "  run_date        DATE NOT NULL,\n",
    "  run_hour        TIME NOT NULL,\n",
    "  magaza          TEXT,\n",
    "  total_collages  INT,\n",
    "  total_items     INT,\n",
    "  rotten_count    INT NOT NULL,\n",
    "  fresh_count     INT NOT NULL,\n",
    "  unknown_count   INT NOT NULL,\n",
    "  model_name      TEXT,\n",
    "  prompt_version  TEXT,\n",
    "  min_conf_rotten REAL,\n",
    "  collages_json   JSONB\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_llm_runs_created_at ON llm_runs(created_at);\n",
    "CREATE INDEX IF NOT EXISTS idx_llm_runs_run_date ON llm_runs(run_date);\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS llm_items (\n",
    "  id           BIGSERIAL PRIMARY KEY,\n",
    "  run_id       BIGINT REFERENCES llm_runs(id) ON DELETE CASCADE,\n",
    "  item_id      TEXT,\n",
    "  urun         TEXT NOT NULL,\n",
    "  magaza       TEXT,\n",
    "  tarih        DATE NOT NULL,\n",
    "  saat         TIME NOT NULL,\n",
    "  durum        TEXT NOT NULL,\n",
    "  dosya        TEXT,\n",
    "  raw_json     JSONB,\n",
    "  inserted_at  TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_llm_items_tarih ON llm_items(tarih);\n",
    "CREATE INDEX IF NOT EXISTS idx_llm_items_durum ON llm_items(durum);\n",
    "CREATE INDEX IF NOT EXISTS idx_llm_items_run_id ON llm_items(run_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_llm_items_urun ON llm_items(urun);\n",
    "\"\"\"\n",
    "\n",
    "def ensure_tables():\n",
    "    \"\"\"VeritabanÄ±nda gerekli tablolarÄ± oluÅŸturur\"\"\"\n",
    "    if not PG_DSN:\n",
    "        raise RuntimeError(\"PG_DSN tanÄ±mlÄ± deÄŸil. VeritabanÄ± baÄŸlantÄ±sÄ± kurulamÄ±yor.\")\n",
    "    \n",
    "    try:\n",
    "        with psycopg2.connect(PG_DSN) as conn:\n",
    "            with conn.cursor() as cur: \n",
    "                cur.execute(DDL)\n",
    "            conn.commit()\n",
    "        print(\"[OK] VeritabanÄ± tablolarÄ± hazÄ±r (llm_runs, llm_items)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[HATA] VeritabanÄ± baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z: {e}\")\n",
    "        raise\n",
    "\n",
    "# --------- En son gÃ¼n/saat klasÃ¶rÃ¼nÃ¼ bul (S3 Object Storage'dan) ----------\n",
    "DATE_RE = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}$\")\n",
    "HOUR_RE = re.compile(r\"^\\d{1,2}$\")\n",
    "\n",
    "def find_all_camera_hour_dirs_from_s3() -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    S3 Object Storage'dan crops/camera_XXX/YYYY-MM-DD/HH/ yapÄ±sÄ±ndaki \n",
    "    TÃœM kamera klasÃ¶rlerinin en son saat klasÃ¶rlerini bulur.\n",
    "    \n",
    "    Returns:\n",
    "        List[Tuple[camera_id, date_name, hour_name]]\n",
    "    \"\"\"\n",
    "    prefix = \"crops/camera_\"\n",
    "    all_blobs = list_blobs_in_path_db(prefix)\n",
    "    \n",
    "    if not all_blobs:\n",
    "        print(\"[!] S3 Object Storage'da crops klasÃ¶rÃ¼ bulunamadÄ±\")\n",
    "        return []\n",
    "    \n",
    "    # Kamera -> Tarih -> Saat yapÄ±sÄ±nÄ± oluÅŸtur\n",
    "    camera_dates_hours = {}  # {camera_id: {date: {hour: []}}}\n",
    "    \n",
    "    for blob_path in all_blobs:\n",
    "        # crops/camera_001/2025-01-27/17/collages/collage_001.llm.json formatÄ±ndan parse et\n",
    "        parts = blob_path.replace(\"crops/\", \"\").split(\"/\")\n",
    "        if len(parts) >= 3 and parts[0].startswith(\"camera_\") and DATE_RE.match(parts[1]) and HOUR_RE.match(parts[2]):\n",
    "            camera_id = parts[0]\n",
    "            date_name = parts[1]\n",
    "            hour_name = parts[2]\n",
    "            \n",
    "            if camera_id not in camera_dates_hours:\n",
    "                camera_dates_hours[camera_id] = {}\n",
    "            if date_name not in camera_dates_hours[camera_id]:\n",
    "                camera_dates_hours[camera_id][date_name] = {}\n",
    "            if hour_name not in camera_dates_hours[camera_id][date_name]:\n",
    "                camera_dates_hours[camera_id][date_name][hour_name] = []\n",
    "            camera_dates_hours[camera_id][date_name][hour_name].append(blob_path)\n",
    "    \n",
    "    if not camera_dates_hours:\n",
    "        print(\"[!] S3 Object Storage'da kamera klasÃ¶rÃ¼ bulunamadÄ±\")\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    # Her kamera iÃ§in en son tarih ve saat klasÃ¶rÃ¼nÃ¼ bul\n",
    "    for camera_id, dates_dict in camera_dates_hours.items():\n",
    "        sorted_dates = sorted(dates_dict.keys())\n",
    "        if not sorted_dates:\n",
    "            continue\n",
    "        last_date = sorted_dates[-1]\n",
    "        \n",
    "        hours_dict = dates_dict[last_date]\n",
    "        sorted_hours = sorted(hours_dict.keys(), key=lambda h: int(h) if h.isdigit() else -1)\n",
    "        if not sorted_hours:\n",
    "            continue\n",
    "        last_hour = sorted_hours[-1]\n",
    "        \n",
    "        results.append((camera_id, last_date, last_hour))\n",
    "    \n",
    "    print(f\"[i] {len(results)} kamera klasÃ¶rÃ¼ bulundu: {', '.join([r[0] for r in results])}\")\n",
    "    return results\n",
    "\n",
    "# --------- YardÄ±mcÄ±lar ----------\n",
    "def _count_statuses(items: List[Dict[str, Any]]) -> Tuple[int, int, int]:\n",
    "    \"\"\"Items listesinden Ã§Ã¼rÃ¼k, saÄŸlÄ±klÄ± ve bilinmeyen sayÄ±larÄ±nÄ± hesaplar\"\"\"\n",
    "    rotten = 0\n",
    "    fresh = 0\n",
    "    unknown = 0\n",
    "    \n",
    "    for item in items:\n",
    "        durum = (item.get(\"durum\") or \"\").strip().lower()\n",
    "        if durum == \"Ã§Ã¼rÃ¼k\":\n",
    "            rotten += 1\n",
    "        elif durum == \"saÄŸlÄ±klÄ±\":\n",
    "            fresh += 1\n",
    "        else:\n",
    "            unknown += 1\n",
    "    \n",
    "    return rotten, fresh, unknown\n",
    "\n",
    "def _load_json_from_blob(s3_key: str) -> Optional[Dict]:\n",
    "    \"\"\"S3'ten JSON dosyasÄ±nÄ± yÃ¼kler\"\"\"\n",
    "    try:\n",
    "        s3_data = download_blob_to_memory(s3_key)\n",
    "        if not s3_data:\n",
    "            return None\n",
    "        return json.loads(s3_data.decode(\"utf-8\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[!] JSON okunamadÄ± ({s3_key}): {e}\")\n",
    "        return None\n",
    "\n",
    "# --------- Insert iÅŸlemleri ----------\n",
    "def insert_run(conn, *, run_date, run_hour, magaza, total_collages, total_items,\n",
    "               rotten_count, fresh_count, unknown_count, model_name, prompt_version,\n",
    "               min_conf_rotten, collages_json) -> int:\n",
    "    \"\"\"Tek bir run kaydÄ± ekler (tÃ¼m collages toplu)\"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO llm_runs\n",
    "              (run_date, run_hour, magaza, total_collages, total_items,\n",
    "               rotten_count, fresh_count, unknown_count, model_name, prompt_version,\n",
    "               min_conf_rotten, collages_json)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s::jsonb)\n",
    "            RETURNING id;\n",
    "        \"\"\", (\n",
    "            run_date, run_hour, magaza, total_collages, total_items,\n",
    "            rotten_count, fresh_count, unknown_count, model_name, prompt_version,\n",
    "            min_conf_rotten,\n",
    "            json.dumps(collages_json, ensure_ascii=False) if collages_json else None\n",
    "        ))\n",
    "        row = cur.fetchone()\n",
    "        return row[0] if row else None\n",
    "\n",
    "def insert_items(conn, run_id: int, items: List[Dict[str, Any]]) -> int:\n",
    "    \"\"\"ÃœrÃ¼n item'larÄ±nÄ± veritabanÄ±na ekler\"\"\"\n",
    "    if not items: return 0\n",
    "    inserted = 0\n",
    "    with conn.cursor() as cur:\n",
    "        for obj in items:\n",
    "            cur.execute(\"\"\"\n",
    "                INSERT INTO llm_items\n",
    "                  (run_id, item_id, urun, magaza, tarih, saat, durum, dosya, raw_json)\n",
    "                VALUES\n",
    "                  (%s, %s, %s, %s, %s, %s, %s, %s, %s::jsonb)\n",
    "            \"\"\", (\n",
    "                run_id,\n",
    "                obj.get(\"id\"),\n",
    "                obj.get(\"urun\"),\n",
    "                obj.get(\"magaza\"),\n",
    "                obj.get(\"tarih\"),\n",
    "                obj.get(\"saat\"),\n",
    "                obj.get(\"durum\"),\n",
    "                obj.get(\"dosya\"),\n",
    "                json.dumps(obj, ensure_ascii=False) if obj else None\n",
    "            ))\n",
    "            inserted += cur.rowcount\n",
    "    return inserted\n",
    "\n",
    "# --------- Ingest akÄ±ÅŸlarÄ± ----------\n",
    "def ingest_hour_directory_from_s3(conn, camera_id: str, date_name: str, hour_name: str) -> Tuple[Optional[int], int]:\n",
    "    \"\"\"\n",
    "    S3 Object Storage'dan bir kameranÄ±n saat klasÃ¶rÃ¼ndeki TÃœM collages/*.llm.json dosyalarÄ±nÄ± \n",
    "    tek bir run olarak iÅŸler.\n",
    "    \n",
    "    Args:\n",
    "        conn: PostgreSQL baÄŸlantÄ±sÄ±\n",
    "        camera_id: Kamera ID'si (Ã¶rn: \"camera_001\")\n",
    "        date_name: Tarih (Ã¶rn: \"2025-01-27\")\n",
    "        hour_name: Saat (Ã¶rn: \"17\")\n",
    "    \n",
    "    Returns: (run_id, total_items_inserted)\n",
    "    \"\"\"\n",
    "    # S3'te collages klasÃ¶rÃ¼ndeki .llm.json dosyalarÄ±nÄ± bul\n",
    "    prefix = f\"crops/{camera_id}/{date_name}/{hour_name}/collages/\"\n",
    "    all_s3_keys = list_blobs_in_path_db(prefix)\n",
    "    \n",
    "    # Sadece .llm.json dosyalarÄ±nÄ± filtrele\n",
    "    llm_json_keys = [k for k in all_s3_keys if k.endswith(\".llm.json\")]\n",
    "    \n",
    "    if not llm_json_keys:\n",
    "        print(f\"[!] {camera_id}: Collages klasÃ¶rÃ¼nde .llm.json dosyasÄ± bulunamadÄ±\")\n",
    "        return None, 0\n",
    "    \n",
    "    print(f\"[i] {camera_id}: {len(llm_json_keys)} adet .llm.json dosyasÄ± bulundu\")\n",
    "    \n",
    "    # TÃ¼m collage verilerini topla\n",
    "    all_items = []\n",
    "    collages_meta = []\n",
    "    model_name = None\n",
    "    prompt_version = None\n",
    "    min_conf_rotten = None\n",
    "    \n",
    "    for s3_key in sorted(llm_json_keys):\n",
    "        data = _load_json_from_blob(s3_key)\n",
    "        if not data:\n",
    "            continue\n",
    "        \n",
    "        # Ä°lk collage'dan meta bilgileri al\n",
    "        if model_name is None:\n",
    "            model_name = data.get(\"model_name\")\n",
    "            prompt_version = data.get(\"prompt_version\")\n",
    "            min_conf_rotten = data.get(\"min_conf_rotten\")\n",
    "        \n",
    "        # Items'larÄ± biriktir\n",
    "        items = data.get(\"items\", [])\n",
    "        all_items.extend(items)\n",
    "        \n",
    "        # Collage meta'sÄ±nÄ± kaydet\n",
    "        collages_meta.append({\n",
    "            \"collage_path\": data.get(\"collage_path\"),\n",
    "            \"collage_blob_path\": data.get(\"collage_blob_path\"),\n",
    "            \"batch_size\": data.get(\"batch_size\"),\n",
    "            \"raw_llm\": data.get(\"raw_llm\"),\n",
    "            \"num_to_class\": data.get(\"num_to_class\")\n",
    "        })\n",
    "    \n",
    "    if not all_items:\n",
    "        print(f\"[!] HiÃ§ item bulunamadÄ±\")\n",
    "        return None, 0\n",
    "    \n",
    "    # DurumlarÄ± say\n",
    "    rotten_count, fresh_count, unknown_count = _count_statuses(all_items)\n",
    "    \n",
    "    run_date = date_name  # YYYY-MM-DD\n",
    "    run_hour = hour_name.zfill(2) + \":00:00\"  # HH:00:00\n",
    "    \n",
    "    # MaÄŸaza bilgisini ilk item'dan al\n",
    "    magaza = all_items[0].get(\"magaza\") if all_items else None\n",
    "    \n",
    "    # Tek bir run kaydÄ± oluÅŸtur\n",
    "    run_id = insert_run(\n",
    "        conn,\n",
    "        run_date        = run_date,\n",
    "        run_hour        = run_hour,\n",
    "        magaza          = magaza,\n",
    "        total_collages  = len(llm_json_blobs),\n",
    "        total_items     = len(all_items),\n",
    "        rotten_count    = rotten_count,\n",
    "        fresh_count     = fresh_count,\n",
    "        unknown_count   = unknown_count,\n",
    "        model_name      = model_name,\n",
    "        prompt_version  = prompt_version,\n",
    "        min_conf_rotten = min_conf_rotten,\n",
    "        collages_json   = collages_meta\n",
    "    )\n",
    "    \n",
    "    if not run_id:\n",
    "        print(f\"[!] Run kaydÄ± oluÅŸturulamadÄ±\")\n",
    "        return None, 0\n",
    "    \n",
    "    # TÃ¼m items'larÄ± bu run_id ile kaydet\n",
    "    inserted = insert_items(conn, run_id, all_items)\n",
    "    \n",
    "    return run_id, inserted\n",
    "\n",
    "# --------- Ana Fonksiyon ----------\n",
    "def write_to_database():\n",
    "    \"\"\"\n",
    "    S3 Object Storage'dan crops/camera_XXX/ klasÃ¶rlerindeki en son tarih/saat klasÃ¶rlerindeki \n",
    "    TÃœM collages/*.llm.json dosyalarÄ±nÄ± PostgreSQL veritabanÄ±na yazar.\n",
    "    \n",
    "    YENÄ° YAPILANMA:\n",
    "    - S3 Object Storage'dan veri Ã§eker\n",
    "    - Her kamera iÃ§in bir saat klasÃ¶rÃ¼ndeki tÃ¼m kolajlar = 1 RUN\n",
    "    - llm_runs: Her kamera iÃ§in bir kayÄ±t (toplam istatistikler)\n",
    "    - llm_items: TÃ¼m Ã¼rÃ¼n kayÄ±tlarÄ± (run_id ile baÄŸlÄ±)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PostgreSQL VeritabanÄ±na Veri AktarÄ±mÄ± (S3 Object Storage)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not PG_DSN:\n",
    "        print(\"[HATA] PG_DSN ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil!\")\n",
    "        print(\"LÃ¼tfen .env dosyasÄ±na ÅŸu formatta ekleyin:\")\n",
    "        print(\"PG_DSN=postgresql://USER:PASS@HOST:5432/DB?sslmode=require\")\n",
    "        return\n",
    "    \n",
    "    if not S3_ACCESS_KEY_ID or not S3_SECRET_ACCESS_KEY:\n",
    "        print(\"[HATA] S3_ACCESS_KEY_ID veya S3_SECRET_ACCESS_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil!\")\n",
    "        print(\"LÃ¼tfen .env dosyasÄ±na S3 credentials ekleyin.\")\n",
    "        return\n",
    "    \n",
    "    # TablolarÄ± oluÅŸtur/kontrol et\n",
    "    try:\n",
    "        ensure_tables()\n",
    "    except Exception as e:\n",
    "        print(f\"[HATA] VeritabanÄ± hazÄ±rlanamadÄ±: {e}\")\n",
    "        return\n",
    "    \n",
    "    # S3 Object Storage'dan tÃ¼m kameralarÄ±n en son saat klasÃ¶rlerini bul\n",
    "    camera_hour_dirs = find_all_camera_hour_dirs_from_s3()\n",
    "    if not camera_hour_dirs:\n",
    "        print(f\"[!] S3 Object Storage'da crops/camera_XXX/<YYYY-MM-DD>/<HH> yapÄ±sÄ±nda klasÃ¶r bulunamadÄ±\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n[i] {len(camera_hour_dirs)} kamera iÃ§in iÅŸlem yapÄ±lacak\\n\")\n",
    "    \n",
    "    # Her kamera iÃ§in veritabanÄ±na yaz\n",
    "    total_runs = 0\n",
    "    total_items = 0\n",
    "    \n",
    "    try:\n",
    "        with psycopg2.connect(PG_DSN) as conn:\n",
    "            for camera_id, date_name, hour_name in camera_hour_dirs:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"[â†’] {camera_id}: {date_name}/{hour_name}\")\n",
    "                print(f\"{'='*70}\")\n",
    "                \n",
    "                run_id, items_inserted = ingest_hour_directory_from_s3(conn, camera_id, date_name, hour_name)\n",
    "                \n",
    "                if run_id:\n",
    "                    total_runs += 1\n",
    "                    total_items += items_inserted\n",
    "                    \n",
    "                    # Ä°statistikleri gÃ¶ster\n",
    "                    with conn.cursor() as cur:\n",
    "                        cur.execute(\"\"\"\n",
    "                            SELECT total_collages, total_items, \n",
    "                                   rotten_count, fresh_count, unknown_count\n",
    "                            FROM llm_runs WHERE id = %s\n",
    "                        \"\"\", (run_id,))\n",
    "                        row = cur.fetchone()\n",
    "                        if row:\n",
    "                            total_col, total_itm, rotten, fresh, unknown = row\n",
    "                            \n",
    "                            print(f\"\\n[âœ“] RUN BAÅARILI - ID: {run_id}\")\n",
    "                            print(f\"  ğŸ“Š Toplam Kolaj     : {total_col}\")\n",
    "                            print(f\"  ğŸ“¦ Toplam ÃœrÃ¼n      : {total_itm}\")\n",
    "                            if total_itm > 0:\n",
    "                                print(f\"  âœ… SaÄŸlÄ±klÄ±         : {fresh} ({fresh/total_itm*100:.1f}%)\")\n",
    "                                print(f\"  âŒ Ã‡Ã¼rÃ¼k            : {rotten} ({rotten/total_itm*100:.1f}%)\")\n",
    "                            else:\n",
    "                                print(f\"  âœ… SaÄŸlÄ±klÄ±         : 0\")\n",
    "                                print(f\"  âŒ Ã‡Ã¼rÃ¼k            : 0\")\n",
    "                            print(f\"  â“ Bilinmeyen       : {unknown}\")\n",
    "                            print(f\"  ğŸ’¾ VeritabanÄ± KayÄ±t : {items_inserted} item\")\n",
    "            \n",
    "            # Commit tÃ¼m kameralar iÃ§in\n",
    "            conn.commit()\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"[âœ“] TÃœM KAMERALAR TAMAMLANDI!\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"  ğŸ“Š Toplam Run       : {total_runs}\")\n",
    "            print(f\"  ğŸ“¦ Toplam Item      : {total_items}\")\n",
    "            print(f\"{'='*70}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[HATA] VeritabanÄ± iÅŸlemi baÅŸarÄ±sÄ±z: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Jupyter notebook'ta Ã§alÄ±ÅŸtÄ±r\n",
    "write_to_database()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}