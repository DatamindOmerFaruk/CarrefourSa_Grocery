"""
S3 Object Storage'dan gÃ¶rselleri alÄ±p API'lara gÃ¶ndererek PostgreSQL'e yazan batch processor
"""
import os
import requests
import psycopg2
from psycopg2.extras import RealDictCursor
import json
import boto3
from botocore.exceptions import ClientError
from botocore.config import Config
from datetime import datetime, timedelta
# urllib3 SSL uyarÄ±larÄ±nÄ± bastÄ±r (self-signed certificate iÃ§in)
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import logging
from typing import List, Dict, Any, Optional
import time
from urllib.parse import quote
from dotenv import load_dotenv

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# AWS checksum hesaplama ve doÄŸrulama iÃ§in environment variable'larÄ± ayarla
# Bu, bazÄ± S3 uyumlu sistemlerde (Cohesity gibi) Content-Length sorunlarÄ±nÄ± Ã§Ã¶zebilir
os.environ.setdefault("AWS_REQUEST_CHECKSUM_CALCULATION", "when_required")
os.environ.setdefault("AWS_RESPONSE_CHECKSUM_VALIDATION", "when_required")

# Logging ayarlarÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_processor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# === PostgreSQL Tablo DDL'leri ===
DDL_TABLES = """
-- Content analiz sonuÃ§larÄ± tablosu
CREATE TABLE IF NOT EXISTS analyze_row (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    source_url TEXT NOT NULL,
    satir_sayisi INT,
    sutun_sayisi INT,
    toplam_kasa INT,
    row_index INT,
    konum TEXT,
    ana_urun TEXT,
    yan_urunler TEXT,
    raw JSONB
);

CREATE INDEX IF NOT EXISTS idx_analyze_row_source_url ON analyze_row(source_url);
CREATE INDEX IF NOT EXISTS idx_analyze_row_created_at ON analyze_row(created_at);
CREATE INDEX IF NOT EXISTS idx_analyze_row_ana_urun ON analyze_row(ana_urun);

-- Stock analiz sonuÃ§larÄ± tablosu
CREATE TABLE IF NOT EXISTS analyze_stock_row (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    source_url TEXT NOT NULL,
    reyon_id TEXT,
    doluluk REAL,
    durum TEXT,
    aciliyet TEXT,
    kasa_gorunurlugu BOOLEAN,
    doluluk_seviyeleri TEXT,
    raw JSONB
);

CREATE INDEX IF NOT EXISTS idx_analyze_stock_row_source_url ON analyze_stock_row(source_url);
CREATE INDEX IF NOT EXISTS idx_analyze_stock_row_created_at ON analyze_stock_row(created_at);
CREATE INDEX IF NOT EXISTS idx_analyze_stock_row_durum ON analyze_stock_row(durum);

-- Evaluation analiz sonuÃ§larÄ± tablosu
CREATE TABLE IF NOT EXISTS analyze_evaluation_row (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    source_url TEXT NOT NULL,
    genel_skor REAL,
    toplam_hata INT,
    kritik_hata INT,
    uyari INT,
    analiz_modu TEXT,
    hata_tipi TEXT,
    konum1 TEXT,
    urun1 TEXT,
    konum2 TEXT,
    urun2 TEXT,
    problem TEXT,
    oneri TEXT,
    olumlu_yerlesimler JSONB,
    genel_oneriler JSONB,
    raw JSONB
);

CREATE INDEX IF NOT EXISTS idx_analyze_evaluation_row_source_url ON analyze_evaluation_row(source_url);
CREATE INDEX IF NOT EXISTS idx_analyze_evaluation_row_created_at ON analyze_evaluation_row(created_at);
CREATE INDEX IF NOT EXISTS idx_analyze_evaluation_row_genel_skor ON analyze_evaluation_row(genel_skor);
"""

class BatchProcessor:
    def __init__(self):
        """KonfigÃ¼rasyonlarÄ± yÃ¼kle"""
        self.load_config()
        self.setup_connections()
        
    def load_config(self):
        """Ã‡evre deÄŸiÅŸkenlerinden konfigÃ¼rasyonlarÄ± yÃ¼kle"""
        # S3 Object Storage
        self.s3_endpoint_url = os.getenv('S3_ENDPOINT_URL', 'https://161cohesity.carrefoursa.com:3000')
        self.s3_access_key_id = os.getenv('S3_ACCESS_KEY_ID', 'sWxdTl3ERx7myBE1qpW06_haVvuhATcdsmBbqaWkXYU')
        self.s3_secret_access_key = os.getenv('S3_SECRET_ACCESS_KEY', 'Ti9Fonk3wYyG5PMx5LaGUmlcVyCuqsE5BLVV5vv8PU0')
        self.s3_bucket_name = os.getenv('S3_BUCKET_NAME', 'Grocery')
        
        # PostgreSQL
        self.pg_host = os.getenv('POSTGRES_HOST', '45.84.18.76')
        self.pg_port = os.getenv('POSTGRES_PORT', '5432')
        self.pg_database = os.getenv('POSTGRES_DB', 'postgres')  # VeritabanÄ± adÄ±: postgres
        self.pg_user = os.getenv('POSTGRES_USER', 'grocerryadmin')  # KullanÄ±cÄ± adÄ±: grocerryadmin
        self.pg_password = os.getenv('POSTGRES_PASSWORD', 'a08Iyr95vLHTYY')
        
        # API Endpoints
        self.api_base_url = os.getenv('API_BASE_URL', 'http://localhost:8000')
        self.test_mode = os.getenv('TEST_MODE', 'false').lower() == 'true'
        
        # Batch ayarlarÄ±
        self.batch_size = int(os.getenv('BATCH_SIZE', '10'))
        self.retry_count = int(os.getenv('RETRY_COUNT', '3'))
        self.delay_between_requests = float(os.getenv('REQUEST_DELAY', '1.0'))
        
        # BaÄŸlantÄ± bilgilerini log'a yazdÄ±r
        logger.info(f"PostgreSQL Host: {self.pg_host}:{self.pg_port}")
        logger.info(f"PostgreSQL Database: {self.pg_database}")
        logger.info(f"PostgreSQL User: {self.pg_user}")
        logger.info(f"API Base URL: {self.api_base_url}")
        
        if not all([self.s3_access_key_id, self.s3_secret_access_key, self.pg_database, self.pg_user, self.pg_password]):
            raise ValueError("Gerekli Ã§evre deÄŸiÅŸkenleri eksik!")
            
    def setup_connections(self):
        """S3 ve PostgreSQL baÄŸlantÄ±larÄ±nÄ± kur"""
        try:
            # S3 Client
            self.s3_client = boto3.client(
                "s3",
                endpoint_url=self.s3_endpoint_url,
                aws_access_key_id=self.s3_access_key_id,
                aws_secret_access_key=self.s3_secret_access_key,
                verify=False,  # self-signed iÃ§in
                config=Config(
                    signature_version="s3v4",
                    s3={"addressing_style": "path"},  # Ã–NEMLÄ°: path style
                ),
            )
            
            # PostgreSQL baÄŸlantÄ±sÄ±
            # sslmode=prefer: SSL varsa kullanÄ±r, yoksa SSL olmadan baÄŸlanÄ±r
            logger.info(f"PostgreSQL'ye baÄŸlanÄ±lÄ±yor: {self.pg_host}:{self.pg_port}/{self.pg_database} (user: {self.pg_user})")
            self.pg_connection = psycopg2.connect(
                host=self.pg_host,
                port=self.pg_port,
                database=self.pg_database,
                user=self.pg_user,
                password=self.pg_password,
                sslmode='prefer'  # SSL desteklenmiyorsa otomatik olarak SSL olmadan baÄŸlanÄ±r
            )
            self.pg_connection.autocommit = True
            
            logger.info(f"âœ… PostgreSQL baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±: {self.pg_host}:{self.pg_port}/{self.pg_database}")
            
            # TablolarÄ± oluÅŸtur
            self.ensure_tables()
            
            logger.info("S3 Object Storage ve PostgreSQL baÄŸlantÄ±larÄ± baÅŸarÄ±lÄ±")
            
            # API health check
            self.check_api_health()
            
        except Exception as e:
            logger.error(f"BaÄŸlantÄ± hatasÄ±: {str(e)}")
            raise
    
    def ensure_tables(self):
        """VeritabanÄ±nda gerekli tablolarÄ± oluÅŸturur"""
        try:
            with self.pg_connection.cursor() as cursor:
                cursor.execute(DDL_TABLES)
            logger.info("âœ… VeritabanÄ± tablolarÄ± kontrol edildi/oluÅŸturuldu")
        except Exception as e:
            logger.error(f"Tablo oluÅŸturma hatasÄ±: {str(e)}")
            raise
    
    def check_api_health(self) -> bool:
        """API'nin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et. BaÅŸarÄ±lÄ±ysa True, baÅŸarÄ±sÄ±zsa False dÃ¶ndÃ¼r."""
        try:
            health_url = f"{self.api_base_url}/health"
            logger.info(f"API health check: {health_url}")
            response = requests.get(health_url, timeout=5)
            if response.status_code == 200:
                logger.info("API saÄŸlÄ±k kontrolÃ¼ baÅŸarÄ±lÄ±")
                return True
            else:
                logger.warning(f"API health check baÅŸarÄ±sÄ±z: Status {response.status_code}")
                return False
        except requests.exceptions.ConnectionError:
            logger.error(f"API'ye baÄŸlanÄ±lamÄ±yor: {self.api_base_url}")
            logger.error("LÃ¼tfen API'nin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun (systemd service: manav-api)")
            logger.error("API kontrolÃ¼ iÃ§in: sudo systemctl status manav-api")
            logger.error("API baÅŸlatmak iÃ§in: sudo systemctl start manav-api")
            return False
        except Exception as e:
            logger.warning(f"API health check hatasÄ±: {str(e)}")
            return False
            
    def get_all_images(self) -> List[Dict[str, str]]:
        """S3 Object Storage'dan tÃ¼m gÃ¶rselleri listele"""
        try:
            # S3'ten tÃ¼m object'leri listele (snapshots prefix'i altÄ±nda)
            prefix = "snapshots/"
            blobs = []
            
            paginator = self.s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=self.s3_bucket_name, Prefix=prefix)
            
            for page in pages:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        obj_key = obj['Key']
                        # Sadece resim dosyalarÄ±nÄ± al
                        if obj_key.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):
                            # S3 URL oluÅŸtur
                            if self.s3_endpoint_url.endswith('/'):
                                s3_url = f"{self.s3_endpoint_url}{self.s3_bucket_name}/{obj_key}"
                            else:
                                s3_url = f"{self.s3_endpoint_url}/{self.s3_bucket_name}/{obj_key}"
                            
                            blobs.append({
                                'name': obj_key,
                                'url': s3_url,
                                'sas_url': s3_url,  # S3'te SAS token gerekmez, direkt URL kullanÄ±lÄ±r
                                'folder': '/'.join(obj_key.split('/')[:-1]) if '/' in obj_key else '',
                                'size': obj.get('Size', 0),
                                'last_modified': obj.get('LastModified')
                            })
                    
            logger.info(f"Toplam {len(blobs)} gÃ¶rsel dosyasÄ± bulundu")
            return blobs
            
        except Exception as e:
            logger.error(f"S3 Object Storage'dan dosya listesi alÄ±namadÄ±: {str(e)}")
            raise
            
    def download_image(self, s3_key: str) -> bytes:
        """GÃ¶rseli S3 Object Storage'dan indir"""
        try:
            response = self.s3_client.get_object(Bucket=self.s3_bucket_name, Key=s3_key)
            return response['Body'].read()
            
        except ClientError as e:
            logger.error(f"GÃ¶rsel indirilemedi ({s3_key}): {str(e)}")
            raise
            
    def call_api(self, endpoint: str, image_bytes: bytes, additional_data: Dict = None) -> Dict:
        """API endpoint'ine gÃ¶rsel gÃ¶nder"""
        url = f"{self.api_base_url}/{endpoint}"
        
        files = {'file': ('image.jpg', image_bytes, 'image/jpeg')}
        data = additional_data or {}
        
        for attempt in range(self.retry_count):
            try:
                logger.info(f"API Ã§aÄŸrÄ±sÄ±: {url}")
                response = requests.post(url, files=files, data=data, timeout=120)
                logger.info(f"Response status: {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"Response content: {response.text}")
                response.raise_for_status()
                return response.json()
                
            except Exception as e:
                logger.warning(f"API Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z (deneme {attempt + 1}/{self.retry_count}): {str(e)}")
                if attempt == self.retry_count - 1:
                    raise
    def call_api_with_url(self, endpoint: str, image_url: str, additional_data: Dict = None) -> Dict:
        """API endpoint'ine S3 URL gÃ¶nder"""
        url = f"{self.api_base_url}/{endpoint}"
        
        data = {'image_url': image_url}
        if additional_data:
            data.update(additional_data)
        
        for attempt in range(self.retry_count):
            try:
                logger.info(f"API Ã§aÄŸrÄ±sÄ±: {url} with URL: {image_url[:100]}...")
                response = requests.post(url, data=data, timeout=120)
                logger.info(f"Response status: {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"Response content: {response.text}")
                response.raise_for_status()
                return response.json()
                
            except Exception as e:
                logger.warning(f"API Ã§aÄŸrÄ±sÄ± baÅŸarÄ±sÄ±z (deneme {attempt + 1}/{self.retry_count}): {str(e)}")
                if attempt == self.retry_count - 1:
                    raise
                time.sleep(2 ** attempt)  # Exponential backoff
                
    def process_content_api(self, sas_url: str, source_url: str) -> Dict:
        """Content API'yi SAS URL ile Ã§aÄŸÄ±r"""
        try:
            result = self.call_api_with_url('analyze/content', sas_url)
            
            if not result.get('success'):
                raise Exception(f"API hatasÄ±: {result.get('error', 'Bilinmeyen hata')}")
                
            return {
                'success': True,
                'data': result['data'],
                'timestamp': result['timestamp']
            }
            
        except Exception as e:
            logger.error(f"Content API hatasÄ± ({source_url}): {str(e)}")
            return {'success': False, 'error': str(e)}
            
    def process_stock_api(self, sas_url: str, source_url: str) -> Dict:
        """Stock API'yi SAS URL ile Ã§aÄŸÄ±r"""
        try:
            result = self.call_api_with_url('analyze/stock', sas_url)
            
            if not result.get('success'):
                raise Exception(f"API hatasÄ±: {result.get('error', 'Bilinmeyen hata')}")
                
            return {
                'success': True,
                'data': result['data'],
                'timestamp': result['timestamp']
            }
            
        except Exception as e:
            logger.error(f"Stock API hatasÄ± ({source_url}): {str(e)}")
            return {'success': False, 'error': str(e)}
            
    def process_evaluation_api(self, sas_url: str, source_url: str, content_data: Dict = None) -> Dict:
        """Evaluation API'yi SAS URL ile Ã§aÄŸÄ±r"""
        try:
            additional_data = {}
            if content_data and content_data.get('success'):
                additional_data['content_data'] = json.dumps(content_data['data'])
                
            result = self.call_api_with_url('analyze/evaluation', sas_url, additional_data)
            
            if not result.get('success'):
                raise Exception(f"API hatasÄ±: {result.get('error', 'Bilinmeyen hata')}")
                
            return {
                'success': True,
                'data': result['data'],
                'timestamp': result['timestamp']
            }
            
        except Exception as e:
            logger.error(f"Evaluation API hatasÄ± ({source_url}): {str(e)}")
            return {'success': False, 'error': str(e)}
            
    def save_content_results(self, source_url: str, content_data: Dict):
        """Content sonuÃ§larÄ±nÄ± analyze_row tablosuna kaydet"""
        if not content_data.get('success'):
            return
            
        try:
            data = content_data['data']
            grid_info = data.get('grid_bilgisi', {})
            tablo_format = data.get('tablo_format', {})
            satirlar = tablo_format.get('satirlar', [])
            
            with self.pg_connection.cursor() as cursor:
                for idx, satir in enumerate(satirlar):
                    cursor.execute("""
                        INSERT INTO analyze_row (
                            source_url, satir_sayisi, sutun_sayisi, toplam_kasa,
                            row_index, konum, ana_urun, yan_urunler, raw
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        source_url,
                        grid_info.get('satir_sayisi'),
                        grid_info.get('sutun_sayisi'),
                        grid_info.get('toplam_kasa'),
                        idx,
                        satir.get('konum', ''),
                        satir.get('ana_urun', ''),
                        satir.get('yan_urunler', ''),
                        json.dumps(data, ensure_ascii=False)
                    ))
                    
            logger.info(f"Content sonuÃ§larÄ± kaydedildi: {len(satirlar)} satÄ±r")
            
        except Exception as e:
            logger.error(f"Content kaydetme hatasÄ±: {str(e)}")
            
    def save_stock_results(self, source_url: str, stock_data: Dict):
        """Stock sonuÃ§larÄ±nÄ± analyze_stock_row tablosuna kaydet - BASÄ°T METÄ°N FORMAT"""
        if not stock_data.get('success'):
            return
            
        try:
            data = stock_data['data']
            reyon_durumlari = data.get('reyon_durumlarÄ±', [])
            ozet = data.get('Ã¶zet', {})
            
            # Basit metin Ã¶zeti oluÅŸtur
            dolu_reyonlar = []
            normal_reyonlar = []
            kritik_reyonlar = []
            bos_reyonlar = []
            
            for reyon in reyon_durumlari:
                urun = reyon.get('Ã¼rÃ¼n', '')
                konum = reyon.get('konum', '')
                durum = reyon.get('durum', '')
                
                if durum == 'dolu':
                    dolu_reyonlar.append(f"{urun} ({konum})")
                elif durum == 'normal':
                    normal_reyonlar.append(f"{urun} ({konum})")
                elif durum == 'kritik':
                    kritik_reyonlar.append(f"{urun} ({konum})")
                elif durum == 'boÅŸ':
                    bos_reyonlar.append(f"BoÅŸ kasa ({konum})")
            
            # Basit metin Ã¶zeti
            doluluk_ozeti = []
            if dolu_reyonlar:
                doluluk_ozeti.append(f"DOLU: {', '.join(dolu_reyonlar)}")
            if normal_reyonlar:
                doluluk_ozeti.append(f"NORMAL: {', '.join(normal_reyonlar)}")
            if kritik_reyonlar:
                doluluk_ozeti.append(f"KRÄ°TÄ°K: {', '.join(kritik_reyonlar)}")
            if bos_reyonlar:
                doluluk_ozeti.append(f"BOÅž: {', '.join(bos_reyonlar)}")
            
            doluluk_metni = " | ".join(doluluk_ozeti)
            
            with self.pg_connection.cursor() as cursor:
                # Tek bir satÄ±r olarak kaydet (Ã¶zet)
                cursor.execute("""
                    INSERT INTO analyze_stock_row (
                        source_url, reyon_id, doluluk, durum, aciliyet,
                        kasa_gorunurlugu, doluluk_seviyeleri, raw
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    source_url,
                    f"Toplam {ozet.get('toplam_kasa', len(reyon_durumlari))} kasa",
                    None,  # Doluluk oranÄ± gerek yok
                    f"Dolu:{ozet.get('dolu_kasa', 0)} Normal:{ozet.get('normal_kasa', 0)} Kritik:{ozet.get('kritik_kasa', 0)} BoÅŸ:{ozet.get('boÅŸ_kasa', 0)}",
                    'orta' if ozet.get('kritik_kasa', 0) > 0 or ozet.get('boÅŸ_kasa', 0) > 0 else 'dÃ¼ÅŸÃ¼k',
                    True,
                    doluluk_metni,  # BASÄ°T METÄ°N FORMAT!
                    json.dumps(data, ensure_ascii=False)
                ))
                    
            logger.info(f"Stock sonuÃ§larÄ± kaydedildi: {len(reyon_durumlari)} reyon (BASÄ°T METÄ°N)")
            logger.info(f"Doluluk Ã¶zeti: {doluluk_metni}")
            
        except Exception as e:
            logger.error(f"Stock kaydetme hatasÄ±: {str(e)}")
            
    def save_evaluation_results(self, source_url: str, evaluation_data: Dict):
        """Evaluation sonuÃ§larÄ±nÄ± analyze_evaluation_row tablosuna kaydet"""
        if not evaluation_data.get('success'):
            return
            
        try:
            data = evaluation_data['data']
            degerlendirme = data.get('degerlendirme_sonucu', {})
            hatalar = data.get('tespit_edilen_hatalar', [])
            olumlu_yerlesimler = data.get('olumlu_yerlesimler', [])
            genel_oneriler = data.get('genel_oneriler', [])
            analiz_modu = data.get('analiz_modu', '')
            
            with self.pg_connection.cursor() as cursor:
                if hatalar:
                    # Her hata iÃ§in ayrÄ± satÄ±r
                    for hata in hatalar:
                        cursor.execute("""
                            INSERT INTO analyze_evaluation_row (
                                source_url, genel_skor, toplam_hata, kritik_hata, uyari,
                                analiz_modu, hata_tipi, konum1, urun1, konum2, urun2,
                                problem, oneri, olumlu_yerlesimler, genel_oneriler, raw
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            source_url,
                            degerlendirme.get('genel_skor'),
                            degerlendirme.get('toplam_hata'),
                            degerlendirme.get('kritik_hata'),
                            degerlendirme.get('uyari'),
                            analiz_modu,
                            hata.get('hata_tipi'),
                            hata.get('konum1'),
                            hata.get('urun1'),
                            hata.get('konum2'),
                            hata.get('urun2'),
                            hata.get('problem'),
                            hata.get('oneri'),
                            json.dumps(olumlu_yerlesimler, ensure_ascii=False),
                            json.dumps(genel_oneriler, ensure_ascii=False),
                            json.dumps(data, ensure_ascii=False)
                        ))
                else:
                    # Hata yoksa tek satÄ±r
                    cursor.execute("""
                        INSERT INTO analyze_evaluation_row (
                            source_url, genel_skor, toplam_hata, kritik_hata, uyari,
                            analiz_modu, olumlu_yerlesimler, genel_oneriler, raw
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        source_url,
                        degerlendirme.get('genel_skor'),
                        degerlendirme.get('toplam_hata'),
                        degerlendirme.get('kritik_hata'),
                        degerlendirme.get('uyari'),
                        analiz_modu,
                        json.dumps(olumlu_yerlesimler, ensure_ascii=False),
                        json.dumps(genel_oneriler, ensure_ascii=False),
                        json.dumps(data, ensure_ascii=False)
                    ))
                    
            logger.info(f"Evaluation sonuÃ§larÄ± kaydedildi: {len(hatalar) or 1} satÄ±r")
            
        except Exception as e:
            logger.error(f"Evaluation kaydetme hatasÄ±: {str(e)}")
            
    def process_single_image_stock_only(self, blob_info: Dict) -> Dict:
        """Tek gÃ¶rseli iÅŸle - SADECE STOCK ANALÄ°ZÄ°"""
        s3_url = blob_info['sas_url']  # S3 URL'i source_url olarak kullan
        blob_name = blob_info['name']
        
        logger.info(f"Stock analizi: {blob_name}")
        logger.info(f"S3 URL: {s3_url[:100]}...")
        
        try:
            # Sadece Stock API'sini Ã§aÄŸÄ±r
            stock_result = self.process_stock_api(s3_url, s3_url)
            
            # API baÅŸarÄ±sÄ±zsa hata dÃ¶ndÃ¼r
            if not stock_result.get('success', False):
                error_msg = stock_result.get('error', 'Bilinmeyen hata')
                logger.error(f"Stock API baÅŸarÄ±sÄ±z ({blob_name}): {error_msg}")
                return {
                    'success': False,
                    'blob_name': blob_name,
                    'source_url': s3_url,
                    'error': error_msg,
                    'stock_success': False
                }
            
            # SonuÃ§larÄ± kaydet (S3 URL'i source_url olarak)
            self.save_stock_results(s3_url, stock_result)
            
            return {
                'success': True,
                'blob_name': blob_name,
                'source_url': s3_url,
                'stock_success': True
            }
            
        except Exception as e:
            logger.error(f"Stock analizi hatasÄ± ({blob_name}): {str(e)}")
            return {
                'success': False,
                'blob_name': blob_name,
                'source_url': s3_url,
                'error': str(e),
                'stock_success': False
            }

    def process_single_image(self, blob_info: Dict) -> Dict:
        """Tek gÃ¶rseli iÅŸle - S3 URL ile"""
        s3_url = blob_info['sas_url']  # S3 URL'i source_url olarak kullan
        blob_name = blob_info['name']
        
        logger.info(f"Ä°ÅŸleniyor: {blob_name}")
        logger.info(f"S3 URL source_url olarak kaydedilecek: {s3_url[:100]}...")
        
        try:
            # API'leri sÄ±rayla Ã§aÄŸÄ±r (S3 URL ile)
            content_result = self.process_content_api(s3_url, s3_url)
            time.sleep(self.delay_between_requests)
            
            stock_result = self.process_stock_api(s3_url, s3_url)
            time.sleep(self.delay_between_requests)
            
            evaluation_result = self.process_evaluation_api(
                s3_url, s3_url, content_result
            )
            
            # SonuÃ§larÄ± kaydet (S3 URL'i source_url olarak)
            self.save_content_results(s3_url, content_result)
            self.save_stock_results(s3_url, stock_result)
            self.save_evaluation_results(s3_url, evaluation_result)
            
            return {
                'success': True,
                'blob_name': blob_name,
                'source_url': s3_url,
                'content_success': content_result.get('success', False),
                'stock_success': stock_result.get('success', False),
                'evaluation_success': evaluation_result.get('success', False)
            }
            
        except Exception as e:
            logger.error(f"GÃ¶rsel iÅŸleme hatasÄ± ({blob_name}): {str(e)}")
            return {
                'success': False,
                'blob_name': blob_name,
                'source_url': s3_url,
                'error': str(e)
            }
            
    def run_batch_processing(self):
        """Ana batch iÅŸlem dÃ¶ngÃ¼sÃ¼"""
        logger.info("Batch iÅŸlemi baÅŸlatÄ±lÄ±yor...")
        
        # API saÄŸlÄ±k kontrolÃ¼
        if not self.check_api_health():
            logger.error("=" * 60)
            logger.error("API'ye baÄŸlanÄ±lamadÄ±! Ä°ÅŸlem durduruluyor.")
            logger.error("=" * 60)
            logger.error("LÃ¼tfen API'yi baÅŸlatÄ±n ve tekrar deneyin:")
            logger.error("  sudo systemctl start manav-api")
            logger.error("  sudo systemctl status manav-api")
            raise ConnectionError(f"API'ye baÄŸlanÄ±lamÄ±yor: {self.api_base_url}")
        
        try:
            # TÃ¼m gÃ¶rselleri listele
            all_images = self.get_all_images()
            
            if not all_images:
                logger.warning("Ä°ÅŸlenecek gÃ¶rsel bulunamadÄ±")
                return
                
            # Ä°statistikler
            total_images = len(all_images)
            processed = 0
            successful = 0
            failed = 0
            
            logger.info(f"Toplam {total_images} gÃ¶rsel iÅŸlenecek")
            
            # Batch'ler halinde iÅŸle
            for i in range(0, total_images, self.batch_size):
                batch = all_images[i:i + self.batch_size]
                batch_num = (i // self.batch_size) + 1
                
                logger.info(f"Batch {batch_num} iÅŸleniyor ({len(batch)} gÃ¶rsel)")
                
                for blob_info in batch:
                    result = self.process_single_image(blob_info)
                    processed += 1
                    
                    if result['success']:
                        successful += 1
                        logger.info(f"[OK] {result['blob_name']} baÅŸarÄ±lÄ± ({processed}/{total_images})")
                    else:
                        failed += 1
                        logger.error(f"[FAIL] {result['blob_name']} baÅŸarÄ±sÄ±z ({processed}/{total_images})")
                        
                    # Progress raporu
                    if processed % 10 == 0:
                        progress = (processed / total_images) * 100
                        logger.info(f"Ä°lerleme: {progress:.1f}% ({processed}/{total_images})")
                        
                # Batch arasÄ± bekleme
                if i + self.batch_size < total_images:
                    logger.info(f"Batch tamamlandÄ±, {self.delay_between_requests}s bekleniyor...")
                    time.sleep(self.delay_between_requests)
                    
            # Final rapor
            logger.info("=" * 60)
            logger.info("BATCH Ä°ÅžLEMÄ° TAMAMLANDI")
            logger.info(f"Toplam iÅŸlenen: {processed}")
            logger.info(f"BaÅŸarÄ±lÄ±: {successful}")
            logger.info(f"BaÅŸarÄ±sÄ±z: {failed}")
            logger.info(f"BaÅŸarÄ± oranÄ±: {(successful/processed)*100:.1f}%")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"Batch iÅŸlemi genel hatasÄ±: {str(e)}")
            raise
            
    def run_stock_only_processing(self):
        """SADECE STOCK ANALÄ°ZÄ° iÃ§in batch iÅŸlem dÃ¶ngÃ¼sÃ¼"""
        logger.info("Stock-Only Batch iÅŸlemi baÅŸlatÄ±lÄ±yor...")
        
        # API saÄŸlÄ±k kontrolÃ¼
        if not self.check_api_health():
            logger.error("=" * 60)
            logger.error("API'ye baÄŸlanÄ±lamadÄ±! Ä°ÅŸlem durduruluyor.")
            logger.error("=" * 60)
            logger.error("LÃ¼tfen API'yi baÅŸlatÄ±n ve tekrar deneyin:")
            logger.error("  sudo systemctl start manav-api")
            logger.error("  sudo systemctl status manav-api")
            raise ConnectionError(f"API'ye baÄŸlanÄ±lamÄ±yor: {self.api_base_url}")
        
        try:
            # TÃ¼m gÃ¶rselleri listele
            all_images = self.get_all_images()
            
            if not all_images:
                logger.warning("Ä°ÅŸlenecek gÃ¶rsel bulunamadÄ±")
                return
                
            # Ä°statistikler
            total_images = len(all_images)
            processed = 0
            successful = 0
            failed = 0
            
            logger.info(f"Toplam {total_images} gÃ¶rsel SADECE STOCK ANALÄ°ZÄ° iÃ§in iÅŸlenecek")
            
            # Batch'ler halinde iÅŸle
            for i in range(0, total_images, self.batch_size):
                batch = all_images[i:i + self.batch_size]
                batch_num = (i // self.batch_size) + 1
                
                logger.info(f"Stock Batch {batch_num} iÅŸleniyor ({len(batch)} gÃ¶rsel)")
                
                for blob_info in batch:
                    result = self.process_single_image_stock_only(blob_info)
                    processed += 1
                    
                    if result['success']:
                        successful += 1
                        logger.info(f"[STOCK OK] {result['blob_name']} baÅŸarÄ±lÄ± ({processed}/{total_images})")
                    else:
                        failed += 1
                        logger.error(f"[STOCK FAIL] {result['blob_name']} baÅŸarÄ±sÄ±z ({processed}/{total_images})")
                        
                    # Progress raporu
                    if processed % 10 == 0:
                        progress = (processed / total_images) * 100
                        logger.info(f"Stock Ä°lerleme: {progress:.1f}% ({processed}/{total_images})")
                        
                    # Request arasÄ± kÄ±sa bekleme
                    time.sleep(1)
                        
                # Batch arasÄ± bekleme
                if i + self.batch_size < total_images:
                    logger.info(f"Stock Batch tamamlandÄ±, {self.delay_between_requests}s bekleniyor...")
                    time.sleep(self.delay_between_requests)
                    
            # Final rapor
            logger.info("=" * 60)
            logger.info("STOCK-ONLY BATCH Ä°ÅžLEMÄ° TAMAMLANDI")
            logger.info(f"Toplam iÅŸlenen: {processed}")
            logger.info(f"BaÅŸarÄ±lÄ±: {successful}")
            logger.info(f"BaÅŸarÄ±sÄ±z: {failed}")
            logger.info(f"BaÅŸarÄ± oranÄ±: {(successful/processed)*100:.1f}%")
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"Stock batch iÅŸlemi genel hatasÄ±: {str(e)}")
            raise
            
    def close_connections(self):
        """BaÄŸlantÄ±larÄ± kapat"""
        try:
            if hasattr(self, 'pg_connection'):
                self.pg_connection.close()
            logger.info("BaÄŸlantÄ±lar kapatÄ±ldÄ±")
        except Exception as e:
            logger.error(f"BaÄŸlantÄ± kapatma hatasÄ±: {str(e)}")


def main():
    """Ana fonksiyon"""
    processor = None
    
    try:
        processor = BatchProcessor()
        
        # KullanÄ±cÄ±dan mode seÃ§imi
        print("\nðŸ” Batch Processor ModlarÄ±:")
        print("1. Tam Analiz (Content + Stock + Evaluation)")
        print("2. Sadece Stock Analizi (HÄ±zlÄ±)")
        
        while True:
            choice = input("\nHangi modu Ã§alÄ±ÅŸtÄ±rmak istiyorsun? (1/2): ").strip()
            if choice == "1":
                logger.info("TAM ANALÄ°Z modu seÃ§ildi")
                processor.run_batch_processing()
                break
            elif choice == "2":
                logger.info("SADECE STOCK ANALÄ°ZÄ° modu seÃ§ildi")
                processor.run_stock_only_processing()
                break
            else:
                print("âŒ GeÃ§ersiz seÃ§im! 1 veya 2 girin.")
        
    except KeyboardInterrupt:
        logger.info("Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
        
    except Exception as e:
        logger.error(f"Ana iÅŸlem hatasÄ±: {str(e)}")
        raise
        
    finally:
        if processor:
            processor.close_connections()


if __name__ == "__main__":
    main()